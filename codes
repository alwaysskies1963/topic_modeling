
import nltk
from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters
import re

nltk.download('punkt')

def advanced_sentence_tokenizer(text):
    punkt_param = PunktParameters()
    expanded_abbreviation_list = [
        'e.g', 'i.e', 'etc', 'mr', 'mrs', 'dr', 'ph.d', 'prof', 'inc', 'ltd',
        'co', 'jr', 'sr', 'vs', 'dept', 'univ', 'assn', 'bros', 'ph', 'md', 
        'do', 'dvm', 'od', 'dds', 'llc', 'llp', 'corp', 'jan', 'feb', 'mar', 
        'apr', 'jun', 'jul', 'aug', 'sep', 'sept', 'oct', 'nov', 'dec', 
        'mon', 'tue', 'tues', 'wed', 'thu', 'thurs', 'fri', 'sat', 'sun', 
        'no', 'vol', 'ed', 'pp', 'est', 'fig', 'eq', 'ex', 'cf', 'ref', 
        'refs', 'ch', 'sec', 'viz', 'al', 'ave', 'blvd', 'st', 'rd', 'mt',
        # Add more abbreviations as needed
    ]
    punkt_param.abbrev_types = set(expanded_abbreviation_list)
    tokenizer = PunktSentenceTokenizer(punkt_param)
    
    # Replace curly quotes with straight quotes
    text = text.replace('“', '"').replace('”', '"').replace("‘", "'").replace("’", "'")
    
    # Remove special characters (optional, based on need)
    # Be careful with this step as it may remove characters that are meaningful in certain contexts
    # For example, keeping periods, commas, question marks, etc., is important for sentence tokenization
    # text = re.sub(r"[^a-zA-Z0-9.,!?\'\"]", " ", text)
    
    # Additional preprocessing to handle specific cases, e.g., protecting decimal points in numbers
    text = re.sub(r'(\d)\.(\d)', r'\1DECIMAL\2', text)
    text = re.sub(r'\n+', '. ', text)  # Replace newlines with periods to separate statements properly
    
    sentences = tokenizer.tokenize(text)
    
    # Post-tokenization: clean up any placeholders or special handling
    sentences = [sent.replace('DECIMAL', '.') for sent in sentences]
    
    return sentences










------------------------------------------------------------------------------------------------------------------------------------

focusing on the rates of false negatives relative to false positives. Remember, this visualization emphasizes the cost of false negatives (missed detections) versus false positives (false alarms), which might be particularly relevant in contexts where missing a positive case has serious implications, like in medical diagnostics or fraud detection.



fpr, tpr, thresholds = roc_curve(y_true, y_scores)
roc_auc = auc(fpr, tpr)

# Plotting the ROC curve with emphasis on FNR
plt.figure()
plt.plot(fpr, 1 - tpr, label='FNR vs. FPR (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [1, 0], linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('False Negative Rate (FNR)')
plt.title('FNR vs. FPR Curve')
plt.legend(loc="lower left")
plt.show()

================================================================================



import numpy as np
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Sample predicted scores and true labels for Model 1 and Model 2
y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])  # Example true labels
y_scores_model1 = np.array([0.9, 0.1, 0.8, 0.72, 0.3, 0.67, 0.24, 0.45, 0.85, 0.15])  # Predicted scores from Model 1
y_scores_model2 = np.array([0.85, 0.2, 0.75, 0.8, 0.25, 0.7, 0.3, 0.4, 0.9, 0.05])  # Predicted scores from Model 2

# Function to plot ROC curve
def plot_roc_curve(y_true, y_scores, title):
    fpr, tpr, thresholds = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)

    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc="lower right")
    plt.show()

# Plot ROC curve for each model
plot_roc_curve(y_true, y_scores_model1, 'ROC Curve for Model 1')
plot_roc_curve(y_true, y_scores_model2, 'ROC Curve for Model 2')



def plot_combined_roc_curve(y_true, y_scores_model1, y_scores_model2):
    fpr1, tpr1, thresholds1 = roc_curve(y_true, y_scores_model1)
    roc_auc1 = auc(fpr1, tpr1)

    fpr2, tpr2, thresholds2 = roc_curve(y_true, y_scores_model2)
    roc_auc2 = auc(fpr2, tpr2)

    plt.figure()
    plt.plot(fpr1, tpr1, color='darkorange', lw=2, label='Model 1 (area = %0.2f)' % roc_auc1)
    plt.plot(fpr2, tpr2, color='green', lw=2, label='Model 2 (area = %0.2f)' % roc_auc2)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Combined ROC Curves')
    plt.legend(loc="lower right")
    plt.show()

# Plot combined ROC curve
plot_combined_roc_curve(y_true, y_scores_model1, y_scores_model2)






















































def find_similar_words(input_text, top_n=10):
    # Process the input text with the spaCy transformer model
    doc = trf_model(input_text)
    
    # Initialize a list to store similar words and phrases
    similar_words = []
    
    # Iterate through tokens in the processed text
    for token in doc:
        # Check if the token has vector representation (e.g., excluding stopwords)
        if token.has_vector:
            # Calculate the similarity score between the input token and all tokens in the vocabulary
            similarity_scores = [token.similarity(other_token) for other_token in doc]
            
            # Sort tokens by similarity score in descending order
            sorted_tokens = sorted(enumerate(similarity_scores), key=lambda x: x[1], reverse=True)
            
            # Extract the top N similar tokens (excluding the input token itself)
            similar_tokens = [doc[i[0]].text for i in sorted_tokens if i[0] != token.i][:top_n]
            
            # Add the similar tokens to the list
            similar_words.extend(similar_tokens)
    
    # Remove duplicates from the list
    similar_words = list(set(similar_words))
    
    return similar_words

-=-===================================----------------55555555555555555555555555555555555555555
def compute_metrics(TP, FN, FP, TN):
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    precision = TP / (TP + FP) if TP + FP != 0 else 0
    recall = TP / (TP + FN) if TP + FN != 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0
    balanced_accuracy = 0.5 * ((TP / (TP + FN) if TP + FN != 0 else 0) + (TN / (TN + FP) if TN + FP != 0 else 0))
    mcc_denom = ((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) ** 0.5
    mcc = (TP * TN - FP * FN) / mcc_denom if mcc_denom != 0 else 0
    fbeta = (1 + 2**2) * (precision * recall) / ((2**2 * precision) + recall) if precision + recall != 0 else 0
    
    return {
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1 Score": f1_score,
        "Balanced Accuracy": balanced_accuracy,
        "MCC": mcc,
        "F-beta (beta=2)": fbeta
    }

metrics_C = compute_metrics(9, 0, 24, 158)

print("Metrics for Model C:")
for metric, value in metrics_C.items():
    print(f"{metric}: {value:.4f}")
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Get input values
TP = int(input("Enter the number of True Positives (TP): "))
TN = int(input("Enter the number of True Negatives (TN): "))
FP = int(input("Enter the number of False Positives (FP): "))
FN = int(input("Enter the number of False Negatives (FN): "))

# Calculate metrics
accuracy = (TP + TN) / (TP + TN + FP + FN)
recall = TP / (TP + FN) if TP + FN != 0 else 0
precision = TP / (TP + FP) if TP + FP != 0 else 0
f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0
total_instances = TP + TN + FP + FN

# Visualize the confusion matrix
confusion_matrix = np.array([[TP, FP], [FN, TN]])
ax = sns.heatmap(confusion_matrix, annot=True, fmt='g', cmap='Blues', cbar=False)
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.xaxis.set_ticklabels(['Positive', 'Negative'])
ax.yaxis.set_ticklabels(['Positive', 'Negative'])
plt.show()

# Display metrics
print(f"\n\n{'Accuracy:':<15} {accuracy:.4f}")
print(f"{'Recall:':<15} {recall:.4f}")
print(f"{'Precision:':<15} {precision:.4f}")
print(f"{'F1-score:':<15} {f1_score:.4f}")
print(f"{'Total instances:':<15} {total_instances}")

=------------------------------===================================-------------------------------------5678090
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load and prepare the multi-class dataset
data_multiclass = pd.read_csv('your_multiclass_data.csv')  # Adjust the file path
X_multi = data_multiclass.drop('label', axis=1)
y_multi = data_multiclass['label']

# Load and prepare the binary classification dataset
data_binary = pd.read_csv('your_binary_data.csv')  # Adjust the file path
X_binary = data_binary.drop('label', axis=1)
y_binary = data_binary['label']

# Standardize the features
scaler = StandardScaler()
X_multi_scaled = scaler.fit_transform(X_multi)
X_binary_scaled = scaler.fit_transform(X_binary)

# Split the multi-class data
X_multi_train, X_multi_temp, y_multi_train, y_multi_temp = train_test_split(X_multi_scaled, y_multi, test_size=0.4, stratify=y_multi, random_state=42)
X_multi_val, X_multi_test, y_multi_val, y_multi_test = train_test_split(X_multi_temp, y_multi_temp, test_size=0.5, stratify=y_multi_temp, random_state=42)

# Split the binary classification data
X_binary_train, X_binary_temp, y_binary_train, y_binary_temp = train_test_split(X_binary_scaled, y_binary, test_size=0.4, stratify=y_binary, random_state=42)
X_binary_val, X_binary_test, y_binary_val, y_binary_test = train_test_split(X_binary_temp, y_binary_temp, test_size=0.5, stratify=y_binary_temp, random_state=42)


from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier

# Initialize multi-class models
xgb_multi_model = XGBClassifier()  # Add parameters as needed
rf_multi_model = RandomForestClassifier()  # Add parameters as needed

# Train multi-class models
xgb_multi_model.fit(X_multi_train, y_multi_train)
rf_multi_model.fit(X_multi_train, y_multi_train)


from sklearn.model_selection import GridSearchCV

# Example: Grid search for XGBoost multi-class model
param_grid_multi_xgb = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 300]
}

grid_search_multi_xgb = GridSearchCV(xgb_multi_model, param_grid_multi_xgb, cv=3, scoring='accuracy', verbose=2)
grid_search_multi_xgb.fit(X_multi_val, y_multi_val)

# Update multi-class XGBoost model with the best parameters
xgb_multi_model = grid_search_multi_xgb.best_estimator_

# Similar tuning for the RandomForest multi-class model...


# Generate multi-class predictions as a feature for the binary classification dataset
multi_predictions_train = xgb_multi_model.predict(X_multi_train)  # You can also try using rf_multi_model
multi_predictions_val = xgb_multi_model.predict(X_multi_val)
multi_predictions_test = xgb_multi_model.predict(X_multi_test)

--------------------------=============---------------------------------------=
from sklearn.preprocessing import OneHotEncoder

# Assuming your categorical variable is in a column named 'category'

# Initialize OneHotEncoder with known categories
encoder = OneHotEncoder(categories=[range(0, 9)], sparse=False)

# Fit the encoder on the entire data (if available) or just use the known categories
# If you have a dataset that contains all categories, use it to fit. Otherwise, the line below is fine.
encoder.fit([[0], [1], [2], [3], [4], [5], [6], [7], [8]])

# Transform the data in train, validation, and test sets
train_encoded = encoder.transform(train['category'].values.reshape(-1, 1))
val_encoded = encoder.transform(val['category'].values.reshape(-1, 1))
test_encoded = encoder.transform(test['category'].values.reshape(-1, 1))

-================================---------------------------


# Add multi-class predictions as a feature to the binary dataset
X_binary_train_with_multi = np.column_stack((X_binary_train, multi_predictions_train))
X_binary_val_with_multi = np.column_stack((X_binary_val, multi_predictions_val))
X_binary_test_with_multi = np.column_stack((X_binary_test, multi_predictions_test))



# Initialize binary classification models
xgb_binary_model = XGBClassifier(scale_pos_weight = sum(y_binary_train == 0) / sum(y_binary_train == 1)) 
rf_binary_model = RandomForestClassifier(class_weight='balanced')

# Train binary models using the modified dataset
xgb_binary_model.fit(X_binary_train_with_multi, y_binary_train)
rf_binary_model.fit(X_binary_train_with_multi, y_binary_train)

Step 6: Hyperparameter Tuning for Binary Models



from sklearn.metrics import classification_report, accuracy_score

# Predict
































-========================----------------43=3444444444444444------------------------------==============================
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load data
data = pd.read_csv('your_data.csv')  # Adjust the file path

# Separate features and target
X = data.drop('label', axis=1)
y = data['label']

# Standardize the features (optional but recommended)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.4, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier

# Initialize models
xgb_model = XGBClassifier(scale_pos_weight = sum(y_train == 0) / sum(y_train == 1))  # Adjust for imbalance
rf_model = RandomForestClassifier(class_weight='balanced')  # Adjust for imbalance

# Train models
xgb_model.fit(X_train, y_train)
rf_model.fit(X_train, y_train)


from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 300]
}

# Grid search
grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='roc_auc', verbose=2)
grid_search.fit(X_val, y_val)

# Update model with best parameters
xgb_model = grid_search.best_estimator_



from sklearn.model_selection import GridSearchCV

# Parameter grid for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create GridSearchCV object for Random Forest
grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=3, scoring='roc_auc', verbose=2)
grid_search_rf.fit(X_val, y_val)

# Update Random Forest model with the best parameters
rf_model = grid_search_rf.best_estimator_


from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 300]
}

# Grid search
grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='roc_auc', verbose=2)
grid_search.fit(X_val, y_val)

# Update model with best parameters
xgb_model = grid_search.best_estimator_


from sklearn.ensemble import VotingClassifier

# Create a voting classifier
voting_clf = VotingClassifier(
    estimators=[('xgb', xgb_model), ('rf', rf_model)],
    voting='soft'
)

# Fit the voting classifier
voting_clf.fit(X_train, y_train)


from sklearn.metrics import classification_report, accuracy_score

# Predictions
y_pred = voting_clf.predict(X_test)

# Evaluation
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))








































-===================================44444444444444444444444444444444444444444444444444444444444
import pandas as pd

# Define the function to read an .xlsm file and read all text-related data from all sheet tabs ignoring null, non-textual, and numeric values.
def read_xlsm_text_advanced(file_path):
    # Load the Excel file
    xl = pd.ExcelFile(file_path)

    # Create an empty list to store text data
    text_data = []

    # Define a function to check if a cell contains text data (ignoring purely numeric and mixed alphanumeric strings)
    def is_text(cell):
        if isinstance(cell, str):
            # Check if all characters in the string are alphabetic or whitespace
            return all(char.isalpha() or char.isspace() for char in cell)
        return False

    # Iterate through each sheet in the workbook
    for sheet in xl.sheet_names:
        # Read the sheet into a pandas DataFrame
        df = xl.parse(sheet)

        # Iterate through the DataFrame to extract text values
        for column in df.columns:
            for item in df[column]:
                # Check if the item is text-related data
                if pd.notnull(item) and is_text(item):
                    # Add the text to the list, stripping leading/trailing whitespace
                    text_data.append(item.strip())

    # Return the list of text data
    return text_data

# Example usage:
# text_from_xlsm = read_xlsm_text_advanced('path_to_file.xlsm')
# This is commented out to prevent an error since there is no file to read in this environment.

-------------------------------======================================5555555555555555555555555555

import spacy

nlp = spacy.load("en_core_web_md")  # Load the medium-sized English model

def find_similar(keyword, threshold=0.89):
    """Find words similar to keyword using spaCy word vectors"""
    
    doc = nlp(keyword)
    query_vector = doc.vector

    similar_words = []
    for word in nlp.vocab:
        if word.has_vector:  # Check if the word has a vector representation
            similarity = query_vector.dot(word.vector)  # Calculate cosine similarity
            if similarity > threshold:
                similar_words.append((word.text, similarity))
    
    similar_words = sorted(similar_words, key=lambda x: x[1], reverse=True)

    print(f"Similar words to {keyword} with similarity > {threshold}:")
    for word, score in similar_words:
        print(f"- {word} ({score:.3f})")

# Example usage:
keyword = "house"
find_similar(keyword)


-------------------------------------------------------------
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel

# Constants
MAX_LEN = 512
EMBEDDING_DIM = 8000
NUM_ASPECTS = 9
DROPOUT_RATE = 0.5

# Custom attention layer
class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1),
                                 initializer="normal")
        self.b = self.add_weight(name="att_bias", shape=(input_shape[1], 1),
                                 initializer="zeros")
        super(AttentionLayer, self).build(input_shape)

    def call(self, x):
        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)
        a = tf.keras.backend.softmax(e, axis=1)
        output = x * a
        return tf.keras.backend.sum(output, axis=1)

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[-1])

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Input Layer
input_text = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name="input_text")

# BERT for contextual embeddings
bert_model = TFBertModel.from_pretrained('bert-base-uncased')
bert_output = bert_model(input_text)[0]

# Attention layer
attention_output = AttentionLayer()(bert_output)

# Dropout layer after attention
attention_output = tf.keras.layers.Dropout(DROPOUT_RATE)(attention_output)

# Custom layer to concatenate attention outputs to desired dimension
concat_layer = tf.keras.layers.Dense(EMBEDDING_DIM, activation='relu')(attention_output)
concat_layer = tf.keras.layers.Dropout(DROPOUT_RATE)(concat_layer)

# ... [Rest of the model remains the same]

# Print model summary
model.summary()



-=--=========================
# Compile the model with F1-Score as a metric
f1_score = tfa.metrics.F1Score(num_classes=3, average='macro')  # For multi-class tasks
binary_f1_score = tfa.metrics.F1Score(num_classes=1, threshold=0.5)  # For binary tasks

model.compile(optimizer='adam', 
              loss=['categorical_crossentropy'] * NUM_ASPECTS + ['binary_crossentropy'] + ['categorical_crossentropy'] * NUM_ASPECTS,
              metrics=[f1_score] * NUM_ASPECTS + [binary_f1_score] + [f1_score] * NUM_ASPECTS)

# Print model summary
model.summary()


def compute_fbeta(y_true, y_pred, beta=0.5):
    # Convert tensors to binary class matrices
    y_true_class = tf.argmax(y_true, axis=-1)
    y_pred_class = tf.argmax(y_pred, axis=-1)

    # Compute TP, FP, FN
    TP = tf.math.count_nonzero(y_pred_class * y_true_class, axis=0)
    FP = tf.math.count_nonzero(y_pred_class * (y_true_class - 1), axis=0)
    FN = tf.math.count_nonzero((y_pred_class - 1) * y_true_class, axis=0)

    # Compute Precision and Recall
    precision = TP / (TP + FP + tf.keras.backend.epsilon())
    recall = TP / (TP + FN + tf.keras.backend.epsilon())

    # Compute F-beta
    fbeta = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall + tf.keras.backend.epsilon())

    # Return the mean F-beta score for the batch
    return tf.reduce_mean(fbeta)


def compute_fbeta(y_true, y_pred, beta=0.5):
    # Cast y_true to the same dtype as y_pred
    y_true = tf.cast(y_true, y_pred.dtype)

    # Convert tensors to binary class matrices
    y_true_class = tf.argmax(y_true, axis=-1)
    y_pred_class = tf.argmax(y_pred, axis=-1)

    # Convert these to float32 for the upcoming computations
    y_true_class = tf.cast(y_true_class, tf.float32)
    y_pred_class = tf.cast(y_pred_class, tf.float32)

    # Compute TP, FP, FN
    TP = tf.math.count_nonzero(y_pred_class * y_true_class, axis=0)
    FP = tf.math.count_nonzero(y_pred_class * (y_true_class - 1), axis=0)
    FN = tf.math.count_nonzero((y_pred_class - 1) * y_true_class, axis=0)

    # Convert TP, FP, FN to float32
    TP = tf.cast(TP, tf.float32)
    FP = tf.cast(FP, tf.float32)
    FN = tf.cast(FN, tf.float32)

    # Compute Precision and Recall
    precision = TP / (TP + FP + tf.keras.backend.epsilon())
    recall = TP / (TP + FN + tf.keras.backend.epsilon())

    # Compute F-beta
    fbeta = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall + tf.keras.backend.epsilon())

    # Return the mean F-beta score for the batch
    return tf.reduce_mean(fbeta)



=--------------------9076875643576243567890-=
zeroshot bart 
import torch
from transformers import pipeline
from multiprocessing import Pool

# Ensure you have a GPU for faster inference (optional but recommended)
device = 0 if torch.cuda.is_available() else -1

# Create the zero-shot classification pipeline
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli", device=device)

def classify(text):
    # Define the possible labels for classification
    candidate_labels = ["label1", "label2", "label3"]  # Replace with your labels
    result = classifier(text, candidate_labels)
    return result["labels"][0]  # Return the top label

if __name__ == "__main__":
    texts = ["Your list of texts to classify..."]  # Replace with your list of texts

    # Use multiprocessing to classify texts in parallel
    with Pool(processes=4) as pool:  # Adjust the number of processes as needed
        results = pool.map(classify, texts)

    print(results)








=-=-=-=-=------===================----------------------------===========================---------------
# in mortgage industry, we have appraisal to a house or recommendation memos for multi-family.
# sometimes the agents writing these appraisals or rec memos, or lender narratives, or sponsor narratives use some language that are prohibited, 
# for example they should not describe the desirability of neighborhood, or any subjective high-level categories such as race, gender, religion, familial status, age, income, disability, neighborhood description related to applicant, neighborhood,
# like that makes it biased and adds subjectivity on report regarding the above mentioned high-level categories. 
# for removing these prohibited terms related to above high-level categories, 
# we need to parse the appraisal report into sentence and them classified each sentence into one of above multi_categories or none of them if the sentence does not contain none of them. 
# for example following phrases such as good neighborhood, Muslim community , potential district, immigrant community, Chinese neighborhood, black street, good school, kids neighborhood, standards neighborhood, poor subdivision, gentrified neighborhood, fast growing district, gay applicant, young owner , ancient landlord, single loan applicant, 24 years old owner, ... 
# that are implicitly has some meaning related to description of neighborhood, applicant, owner,  ... . 

multi_categories = ['race / ethnicity / national origin / color skin / ... descriptors',
'religion',
'gender identity / sexual orientation / sex / ... descriptors',
'familial status / marital status / familial descriptors',
'neighborhood / community / district / block / subdivision / town / zone / urban / rural / suburban / village / tenants / families / enclave / pockets / ... descriptors',
'age demographics / age descriptors',
'income descriptors and financial status',
'disability descriptors',
'None of above']

binary_category = ['baised', 'not_baised']

# for each sentence in the report, we need to classify it into one of above categories or none of them.
# for example:
# sentence = 'the neighborhood is good'
# category = 'neighborhood / community / district / block / subdivision / town / zone / urban / rural / suburban / village / tenants / families / enclave / pockets / ... descriptors'

# load the a dataframe contains columns such as document name, text, and their categories and binary category from a csv file
import pandas as pd
import torch
import re
import spacy
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from torch.utils.data import DataLoader
import torch
import torch.optim as optim

df = pd.read_csv('MF_prohibited.csv')
df.head()

# we need to clean the text column by removing white space and replace them with space, remove urls, non-ascii characters. 
# we need to convert entities in the text into date_, loc_, org_, person_, time_, money_, percent_, gpe_ , etc. 
# write the function with regex for above rules and apply them on text column.
def text_processing():
    # remove white space
    text = re.sub(r'\s+', ' ', text)
    # remove urls
    text = re.sub(r"http\S+", "", text)
    # remove non-ascii characters
    text = ''.join([i if ord(i) < 128 else ' ' for i in text])
    # convert entities into date_, loc_, org_, person_, time_, money_, percent_, gpe_ , etc.
    doc = nlp(text)
    for ent in doc.ents:
        text = text.replace(ent.text, ent.label_)
    # ...
    return text

df.text.apply(lambda x: text_processing(x))

# load the following huggingface transformers and sentence transformers models for embedding the text into vectors 
# bert-base-uncased, bart-large-mnli, sentence transformers all-MiniLM-L6-V2

model_name1 = 'bert-base-uncased'
model_name2 = 'bart-large-mnli'
model_name3 = 'setence-transformers/all-MiniLM-L6-V2'
tokenizer1 = AutoTokenizer.from_pretrained(model_name1)
tokenizer2 = AutoTokenizer.from_pretrained(model_name2)
tokenizer3 = AutoTokenizer.from_pretrained(model_name3)
model1 = AutoModel.from_pretrained(model_name1)
model2 = AutoModel.from_pretrained(model_name2)
model3 = AutoModel.from_pretrained(model_name3)

# convert the text into sentences, and then use the above models to embed the sentences into vectors
# load the following models for classification
# bert-base-uncased, bart-large-mnli, sentence transformers all-MiniLM-L6-V2
# and then fine-tune them on the above data

def convert_text_into_sentences(text):
    # convert the text into sentences
    # use sentence transformers to parse the text into sentences
    # use spacy en-web-core-trf model to parse the text into sentences
    nlp = spacy.load("en_core_web_trf")
    doc = nlp(text)
    sentences = [sent.text for sent in doc.sents]

    return sentences

# use input_attention_mask and model output to average and max pooling the outputs with tensor size of 128
# write a max_pooling and average_pooling function, the input is model_output and input_attention_mask
def pooling(input_attention_mask, model_output):
    # use input_attention_mask to mask the model_output
    # use max pooling and average pooling to convert the model_output into a single vector representation for the entire sequence (sentence)
    input_attention_mask = input_attention_mask.unsqueeze(-1).expand(model_output.size()).float()
    model_output = model_output * input_attention_mask
    max_pooling = torch.max(model_output, 1)[0]
    average_pooling = torch.mean(model_output, 1)
    # concate both max pooling and average pooling
    pooling = torch.cat((max_pooling, average_pooling), 1)
    return pooling

def pool_embeddings(embeddings, attention_mask):
    # Max pooling
    max_pooled = torch.max(embeddings, dim=1)[0]

    # Average pooling using attention mask
    sum_embeddings = torch.sum(embeddings * attention_mask.unsqueeze(-1), dim=1)
    avg_pooled = sum_embeddings / attention_mask.sum(dim=1, keepdim=True)

    # Concatenate pooled features
    combined = torch.cat([max_pooled, avg_pooled], dim=1)

    return combined

# data object for training the model use def convert_text_into_sentences(text) function
#Pooling strategies are often employed to convert token-level embeddings into a single vector representation for the entire sequence (sentence). Here, we'll use a combination of average and max pooling, taking into account the attention mask to ensure we don't pool over padding tokens.
# use attention mask to ensure we don't pool over padding tokens
class Data(torch.utils.data.Dataset):

    
    def __init__(self, df, tokenizer1, tokenizer2, tokenizer3, model1, model2, model3):
        self.text = df.text.values
        self.sentences = []
        for text in self.text:
            sentences = convert_text_into_sentences(text)
            self.sentences.append(sentences)
        self.categories = df.categories.values
        self.binary_category = df.binary_category.values
        self.tokenizer1 = tokenizer1
        self.tokenizer2 = tokenizer2
        self.tokenizer3 = tokenizer3
        self.model1 = model1
        self.model2 = model2
        self.model3 = model3

    def __len__(self):
        return len(self.text)

    def __getitem__(self, idx):
        text = self.text[idx]
        sentences = self.sentences[idx]
        categories = self.categories[idx]
        binary_category = self.binary_category[idx]

        # embed the sentences into vectors
        embeddings_sentences1 = []
        embeddings_sentences2a = []
        embeddings_sentences2b = []
        embeddings_sentences3 = []
        embeddings_stack = []
        for sentence in sentences:
            # use max-length of sentence to pad the sentence, padding =True, truncation=True, max_length=512            
            input_ids1 = self.tokenizer1(sentence, padding=True, truncation=True, max_length=512, return_tensors="pt")
            input_ids2 = self.tokenizer2(sentence, padding=True, truncation=True, max_length=512, return_tensors="pt")
            input_ids3 = self.tokenizer3(sentence, padding=True, truncation=True, max_length=512, return_tensors="pt")
            with torch.no_grad():
                model_output1 = self.model1(**input_ids1)['last_hidden_state']
                model_output2a = self.model2(**input_ids2)['last_hidden_state']
                model_output2b = self.model2(**input_ids2)['encoder_last_hidden_state']
                model_output3 = self.model3(**input_ids3)['last_hidden_state']
                        
            # use max pooling and average pooling and convert 
            embeddings_sentences1.append(pooling(input_ids1['attention_mask'], model_output1))
            embeddings_sentences2a.append(pooling(input_ids2['attention_mask'], model_output2a))
            embeddings_sentences2b.append(pooling(input_ids2['attention_mask'], model_output2b))
            embeddings_sentences3.append(pooling(input_ids3['attention_mask'], model_output3))
            stack = torch.stack((embeddings_sentences1, embeddings_sentences2a, embeddings_sentences2b, embeddings_sentences3), dim=1)
            embeddings_stack.appedn(torch.mean(stack, dim=1))
                        
        return text, sentences, categories, binary_category, embeddings_stack

# Step 3: Multi-Task Deep Neural Network
# The architecture of the neural network would be:

# An input layer that accepts the concatenated embeddings from the three models.
# A few dense layers for feature extraction.
# Three branches:
# a. Multi-label classification for the high-level categories.
# b. Binary classification for biased/non-biased sentences.
# c. Question-answering task to identify the biased section of the sentence (this is more complex and may require additional considerations).
class MultiTaskModel(nn.Module):
    def __init__(self, embedding_size, num_categories):
        super(MultiTaskModel, self).__init__()

        self.feature_extractor = nn.Sequential(
            nn.Linear(embedding_size, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.5)
        )

        # Multi-label classification branch
        self.multi_label_classifier = nn.Linear(256, num_categories)

        # Binary classification branch
        self.binary_classifier = nn.Linear(256, 1)

        # For QA task, a more sophisticated mechanism is needed. 
        # For the sake of this explanation, I'm using a placeholder.
        self.qa_task = nn.Linear(256, 2)

    def forward(self, x):
        x = self.feature_extractor(x)

        multi_label_output = self.multi_label_classifier(x)
        binary_output = self.binary_classifier(x)
        qa_output = self.qa_task(x)

        return multi_label_output, binary_output, qa_output
    
# Data Preparation
# Assuming you have a PyTorch dataset, you can use the DataLoader for batching
# Instantiate the dataset and dataloader
dataset = AppraisalDataset(your_dataframe, 'text_column_name', 'label_column_name')
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
# Training Loop
# Instantiate the model
model = MultiTaskModel(embedding_size=768*3, num_categories=number_of_high_level_categories)  # Assuming 768 dims for each model's embeddings
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion_multi_label = nn.BCEWithLogitsLoss()  # BCEWithLogitsLoss is typically used for multi-label tasks
criterion_binary = nn.BCEWithLogitsLoss()

num_epochs = 5
for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()

        # Extract embeddings
        embeddings = get_embeddings(batch['text'])
        
        # Forward pass
        multi_label_output, binary_output, _ = model(embeddings)

        # Compute loss
        loss_multi_label = criterion_multi_label(multi_label_output, batch['multi_label_targets'])
        loss_binary = criterion_binary(binary_output, batch['binary_targets'])
        
        # Combine the two losses
        combined_loss = loss_multi_label + loss_binary
        combined_loss.backward()
        
        optimizer.step()
# Note: This is a high-level outline. The actual implementation would require handling various details, such as:
# Moving data and the model to the GPU (if available).
# Tracking and logging metrics.
# Implementing validation and early stopping.
# Adjusting hyperparameters.
# For inference:
def infer(sentence):
    model.eval()
    with torch.no_grad():
        embeddings = get_embeddings(sentence)
        multi_label_pred, binary_pred, qa_pred = model(embeddings)
    
    # Convert predictions to desired format
    # ...

    return multi_label_pred, binary_pred, qa_pred


-----------------------------------------------------------------------------------------------
import torch
from transformers import BartTokenizer, BartForSequenceClassification, BartForQuestionAnswering
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from sklearn.model_selection import train_test_split
import numpy as np

# Define model names and paths
model_name = "facebook/bart-large-mnli"
saved_model_path = "./saved_model"  # Replace with your desired model save path

# Load a pre-trained BART model for sequence classification
tokenizer = BartTokenizer.from_pretrained(model_name)
classification_model = BartForSequenceClassification.from_pretrained(model_name, num_labels=num_categories)

# Sample data
texts = ["Sample text 1", "Sample text 2", "Sample text 3"]  # Replace with your text data
labels = [[1, 0, 1], [0, 1, 0], [1, 1, 0]]  # Replace with your labels, using 1 for positive classes and 0 for others

# Tokenize and format your annotated data for classification
def tokenize_and_format_data(texts, labels):
    input_ids = []
    attention_masks = []
    encoded_labels = []

    for text, label in zip(texts, labels):
        encoding = tokenizer(text, truncation=True, padding=True, return_tensors="pt")
        input_ids.append(encoding.input_ids)
        attention_masks.append(encoding.attention_mask)
        encoded_labels.append(label)

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    encoded_labels = torch.tensor(encoded_labels)

    return input_ids, attention_masks, encoded_labels

input_ids, attention_masks, labels = tokenize_and_format_data(texts, labels)

# Split data into training and validation sets
train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(
    input_ids, attention_masks, labels, test_size=0.2, random_state=42
)

# Create data loaders
batch_size = 32  # Adjust as needed
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)

# Fine-tune the classification model
# You can use the code from previous responses for fine-tuning

# Save the fine-tuned classification model
classification_model.save_pretrained(saved_model_path)
tokenizer.save_pretrained(saved_model_path)

# Step 2: Question Answering

# Load the fine-tuned classification model
qa_model = BartForQuestionAnswering.from_pretrained(saved_model_path)

def find_keywords_and_phrases(texts, class_names):
    keyword_ranges = []

    for text, class_name in zip(texts, class_names):
        # Construct an auxiliary question
        question = f"Which part of the text relates to {class_name}?"

        # Tokenize the text and question
        inputs = tokenizer.encode(question, text, return_tensors="pt", max_length=256, truncation=True)

        # Use the QA model to predict the answer (i.e., keywords or phrases)
        with torch.no_grad():
            outputs = qa_model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask)

        # Extract the predicted answer
        start_index = torch.argmax(outputs.start_logits, dim=1).item()
        end_index = torch.argmax(outputs.end_logits, dim=1).item()

        # Get the corresponding text span
        keyword_span = text[inputs.input_ids[0][start_index:end_index + 1]]

        keyword_ranges.append((class_name, keyword_span))

    return keyword_ranges

# Use the combined model for question answering
classification_model.eval()
keyword_ranges = find_keywords_and_phrases(texts, class_names)

# Print the results
for class_name, keyword_span in keyword_ranges:
    print(f"Class: {class_name}")
    print(f"Keywords/Phrases: {keyword_span}")
    print("\n")

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
import torch
from transformers import BertTokenizer, BertForSequenceClassification, BartTokenizer, BartForQuestionAnswering
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from sklearn.model_selection import train_test_split

# Define model names and paths
classification_model_name = "bert-base-uncased"  # Replace with your classification model name
qa_model_name = "facebook/bart-large-cnn"  # Replace with your QA model name
saved_model_path = "./saved_model"  # Replace with your desired model save path

# Load a pre-trained BERT model for sequence classification
classification_tokenizer = BertTokenizer.from_pretrained(classification_model_name)
classification_model = BertForSequenceClassification.from_pretrained(classification_model_name, num_labels=num_categories)

# Load a pre-trained BART model for question answering
qa_tokenizer = BartTokenizer.from_pretrained(qa_model_name)
qa_model = BartForQuestionAnswering.from_pretrained(qa_model_name)

# Sample data
texts = ["Sample text 1", "Sample text 2", "Sample text 3"]  # Replace with your text data
class_names = ["Class 1", "Class 2", "Class 3"]  # Replace with your class names

# Step 1: Classification

# Tokenize and format your annotated data
def tokenize_and_format_data(texts, labels):
    input_ids = []
    attention_masks = []
    encoded_labels = []

    for text, label in zip(texts, labels):
        encoding = classification_tokenizer(text, truncation=True, padding=True, return_tensors="pt")
        input_ids.append(encoding.input_ids)
        attention_masks.append(encoding.attention_mask)
        encoded_labels.append(label)

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    encoded_labels = torch.tensor(encoded_labels)

    return input_ids, attention_masks, encoded_labels

input_ids, attention_masks, labels = tokenize_and_format_data(texts, class_names)

# Split data into training and validation sets
train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(
    input_ids, attention_masks, labels, test_size=0.2, random_state=42
)

# Create data loaders
batch_size = 32  # Adjust as needed
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)

# Fine-tune the classification model (use the code from previous responses)

# Step 2: Question Answering

def find_keywords_and_phrases(texts, class_names):
    keyword_ranges = []

    for text, class_name in zip(texts, class_names):
        # Tokenize the text and class name
        inputs = qa_tokenizer(class_name, text, return_tensors="pt")

        # Use the QA model to predict the answer (i.e., keywords or phrases)
        with torch.no_grad():
            outputs = qa_model(**inputs)

        # Extract the start and end positions of the answer
        start_index = torch.argmax(outputs.start_logits, dim=1).item()
        end_index = torch.argmax(outputs.end_logits, dim=1).item()

        # Get the corresponding text span
        keyword_span = text[inputs.input_ids[0][start_index:end_index + 1]]

        keyword_ranges.append((class_name, keyword_span))

    return keyword_ranges

# Use the combined model
for epoch in range(num_epochs):  # You can define num_epochs as needed
    classification_model.train()
    total_loss = 0

    for batch in train_dataloader:
        inputs, masks, labels = batch
        outputs = classification_model(inputs, attention_mask=masks, labels=labels)
        loss = outputs.loss

        loss.backward()
        total_loss += loss.item()

        torch.nn.utils.clip_grad_norm_(classification_model.parameters(), max_grad_norm)

        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    # Calculate average training loss for this epoch
    avg_train_loss = total_loss / len(train_dataloader)

    # Validation
    classification_model.eval()
    val_loss = 0

    for batch in val_dataloader:
        inputs, masks, labels = batch
        with torch.no_grad():
            outputs = classification_model(inputs, attention_mask=masks, labels=labels)
            val_loss += outputs.loss.item()

    # Calculate average validation loss for this epoch
    avg_val_loss = val_loss / len(val_dataloader)

    # Print training and validation loss for this epoch
    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')

    # Step 2: Question Answering
    keyword_ranges = find_keywords_and_phrases(texts, class_names)

    # Print the results
    for class_name, keyword_span in keyword_ranges:
        print(f"Class: {class_name}")
        print(f"Keywords/Phrases: {keyword_span}")
        print("\n")

# Save the combined model
classification_model.save_pretrained(saved_model_path)
qa_model.save_pretrained(saved_model_path)
classification_tokenizer.save_pretrained(saved_model_path)
qa_tokenizer.save_pretrained(saved_model_path)

---------------------------------------------------------------------------------------------
from transformers import BartForSequenceClassification, BartTokenizer

model_name = "facebook/bart-large-mnli"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForSequenceClassification.from_pretrained(model_name, num_labels=num_categories)
def tokenize_and_format_data(texts, labels):
    formatted_data = []

    for text, label in zip(texts, labels):
        inputs = tokenizer.encode("classification: " + text, truncation=True, padding=True, return_tensors="pt")
        formatted_data.append((inputs, label))

    return formatted_data

formatted_data = tokenize_and_format_data(texts, labels)
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset

train_data, val_data = train_test_split(formatted_data, test_size=0.2, random_state=42)

train_inputs = torch.stack([item[0] for item in train_data])
train_labels = torch.tensor([item[1] for item in train_data])

val_inputs = torch.stack([item[0] for item in val_data])
val_labels = torch.tensor([item[1] for item in val_data])

train_dataset = TensorDataset(train_inputs, train_labels)
train_sampler = RandomSampler(train_dataset)
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)

val_dataset = TensorDataset(val_inputs, val_labels)
val_sampler = SequentialSampler(val_dataset)
val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)


from transformers import AdamW, get_linear_schedule_with_warmup

# Define optimizer and learning rate scheduler
optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)

# Training loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch in train_dataloader:
        inputs, labels = batch
        outputs = model(inputs, labels=labels)
        loss = outputs.loss

        loss.backward()
        total_loss += loss.item()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)

        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    # Calculate average training loss for this epoch
    avg_train_loss = total_loss / len(train_dataloader)

    # Validation
    model.eval()
    val_loss = 0

    for batch in val_dataloader:
        inputs, labels = batch
        with torch.no_grad():
            outputs = model(inputs, labels=labels)
            val_loss += outputs.loss.item()

    # Calculate average validation loss for this epoch
    avg_val_loss = val_loss / len(val_dataloader)

    # Print training and validation loss for this epoch
    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')


model.save_pretrained(saved_model_path)
tokenizer.save_pretrained(saved_model_path)

=============================================================================================

from transformers import BertTokenizer, BertForSequenceClassification

# Load the pre-trained model and tokenizer
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_categories)
# Tokenize and format your data
def tokenize_and_format_data(texts, labels):
    input_ids = []
    attention_masks = []
    encoded_labels = []

    for text, label in zip(texts, labels):
        encoding = tokenizer(text, truncation=True, padding=True, return_tensors="pt")
        input_ids.append(encoding.input_ids)
        attention_masks.append(encoding.attention_mask)
        encoded_labels.append(label)

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    encoded_labels = torch.tensor(encoded_labels)

    return input_ids, attention_masks, encoded_labels

input_ids, attention_masks, labels = tokenize_and_format_data(texts, labels)
from sklearn.model_selection import train_test_split

train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(
    input_ids, attention_masks, labels, test_size=0.2, random_state=42
)
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)
from transformers import AdamW, get_linear_schedule_with_warmup

# Define optimizer and learning rate scheduler
optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)

# Training loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    
    for batch in train_dataloader:
        # Forward pass
        inputs, masks, labels = batch
        outputs = model(inputs, attention_mask=masks, labels=labels)
        loss = outputs.loss
        
        # Backward pass
        loss.backward()
        total_loss += loss.item()
        
        # Gradient clipping if needed
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        
        # Update parameters
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    # Calculate average training loss for this epoch
    avg_train_loss = total_loss / len(train_dataloader)

    # Validation
    model.eval()
    val_loss = 0

    for batch in val_dataloader:
        inputs, masks, labels = batch
        with torch.no_grad():
            outputs = model(inputs, attention_mask=masks, labels=labels)
            val_loss += outputs.loss.item()

    # Calculate average validation loss for this epoch
    avg_val_loss = val_loss / len(val_dataloader)

    # Print training and validation loss for this epoch
    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')
model.save_pretrained(saved_model_path)
tokenizer.save_pretrained(saved_model_path)




-======================-----------------------------------------------------------------------==================================
import pandas as pd

# Example DataFrame
data = {
    'Column1': ['data1', 'data2'],  # Non-list column
    'Column2': ['info1', 'info2'],  # Non-list column
    'ListColumn1': [[1, 2, 3], [4, 5, 6]],  # List column
    'ListColumn2': [['A', 'B', 'C'], ['D', 'E', 'F']],  # List column
    'ListColumn3': [['X', 'Y', 'Z'], ['U', 'V', 'W']]   # List column
}

df = pd.DataFrame(data)

# Exploding the DataFrame
df_exploded = pd.DataFrame({
    'Column1': df['Column1'].repeat(df['ListColumn1'].str.len()),
    'Column2': df['Column2'].repeat(df['ListColumn1'].str.len()),
    'ListColumn1': [item for sublist in df['ListColumn1'] for item in sublist],
    'ListColumn2': [item for sublist in df['ListColumn2'] for item in sublist],
    'ListColumn3': [item for sublist in df['ListColumn3'] for item in sublist]
}).reset_index(drop=True)

print(df_exploded)

-----------------------------------------------------------------------------------------------------------
data = {
    'Column1': [[1, 2, 3], [4, 5, 6]],
    'Column2': [['a', 'b', 'c'], ['d', 'e', 'f']],
    'Column3': [['x', 'y', 'z'], ['u', 'v', 'w']]
}

df = pd.DataFrame(data)

# Ensure all lists in each row are of the same length
# Explode the DataFrame
df['combined'] = list(zip(df['Column1'], df['Column2'], df['Column3']))
df_exploded = df.explode('combined')

# Create separate columns from the tuples
df_exploded[['Column1', 'Column2', 'Column3']] = pd.DataFrame(df_exploded['combined'].tolist(), index=df_exploded.index)

# Drop the combined column if not needed
df_exploded = df_exploded.drop('combined', axis=1)

print(df_exploded)




Background, Purpose, and Scope:
Fannie Mae, a leading provider of mortgage financing, receives thousands of recommendation memos, sponsor credit reviews, and sponsor and lender narratives as part of the loan underwriting process for multifamily properties. This voluminous documentation is essential for assessing the collateral worth of these properties. However, not all the information contained within these reports is necessarily useful for creating an accurate property valuation. In some instances, the appraisals may include subjective opinions or biases based on protected characteristics under fair lending laws and regulations, and these biases can manifest in non-obvious ways.
To address this issue, Fannie Mae's MF Lender Risk Management team has been manually reviewing these reports since 2021, identifying and flagging language that violates the Fannie Mae Selling Guide or contains subjective terminology that should not affect the property's value. While this manual review process has been effective, it is time-consuming and represents a significant cost to the organization.
The purpose of this project is to develop an internal solution that can automatically scan multifamily appraisal comments and detect prohibited language within them. This solution is closely related to the Automated Property Linguistic Detection (APLD) model developed for Fannie Mae's Collateral Risk team, which addresses similar challenges in the single-family mortgage market.
The scope of this project is to create a robust, two-stage AI-based solution, the Multifamily Prohibited Language Detection (MFPLD) model, that can efficiently and reliably identify prohibited language in multifamily appraisal comments, reducing the workload for manual review and ensuring fair and equitable collateral valuation.
Model Usage
The MFPLD model is designed to be used by Fannie Mae's MF Lender Risk Management team to automate the process of detecting prohibited language in multifamily appraisal comments. The model will receive a list of biased language categories from the Fair Lending team, as well as the appraisal files to be analyzed.
In the first stage of the model, the MFPLD model will parse the appraisal reports into sentences and compare them to a pre-defined keyword list provided by the Fair Lending team. This will allow the model to identify any sentences that contain prohibited language related to the input categories.
In the second stage, a text classification model will be used to reduce the number of false positives and false negatives in the results of the first stage. This two-stage approach is similar to the APLD model developed for the Collateral Risk team and is designed to create an effective solution that significantly reduces or eliminates the need for manual review of appraisals for potential bias.
Scope of Model Changes:
The MFPLD model is designed to be a flexible and adaptable solution that can evolve over time to meet the changing needs of Fannie Mae's MF Lender Risk Management team. The model's scope can be expanded to include additional biased language categories, as identified by the Fair Lending team, or to incorporate new sources of appraisal data as they become available.
Furthermore, the text classification model used in the second stage of the MFPLD solution can be fine-tuned and retrained over time to improve its accuracy in detecting false positives and false negatives. This ongoing model refinement will ensure that the MFPLD solution remains effective and efficient in identifying prohibited language in multifamily appraisal comments.
Model Overview:
The MFPLD model is a two-stage AI-based solution that is designed to automatically detect prohibited language in multifamily appraisal comments. The key components of the model are as follows:
Stage 1: Keyword-based Detection
The model receives a list of biased language categories from the Fair Lending team.
The model parses the appraisal reports into sentences and compares them to a pre-defined keyword list provided by the Fair Lending team.
The model identifies any sentences that contain prohibited language related to the input categories.
Stage 2: Text Classification
A text classification model is used to analyze the results of the first stage and identify any false positives or false negatives.
The text classification model is trained on a dataset of appraisal comments, both those containing prohibited language and those that do not, to improve the overall accuracy of the MFPLD solution.
The two-stage approach of the MFPLD model is designed to provide a comprehensive and reliable solution for detecting prohibited language in multifamily appraisal comments. By combining keyword-based detection with machine learning-based text classification, the model can efficiently and accurately identify biased language, reducing the workload for manual review and ensuring fair and equitable collateral valuation.
1. Background, Purpose, and Scope
Background
Fannie Mae is a pivotal entity in the housing finance system, tasked with the significant responsibility of underwriting multifamily property loans. This process requires the examination of thousands of documents annually, including recommendation memos, sponsor credit reviews, and sponsor and lender narrative reports. These documents are crucial for assessing the collateral worth of properties, yet they often contain extraneous or biased information that can skew property valuations. Recognizing the potential for subjective opinions and biases, especially those infringing upon fair lending laws by focusing on protected characteristics, Fannie Mae has historically engaged in manual reviews to identify and address prohibited language within these appraisals.
Purpose
The core aim of introducing an automated solution is to streamline the identification of biased or prohibited language in appraisal comments. This initiative not only seeks to enhance the efficiency and reliability of the process but also aims to mitigate the risk of bias affecting property valuations. By automating this critical review process, Fannie Mae intends to uphold the integrity of its underwriting process while significantly reducing the operational burden and associated costs of manual reviews.
Scope
The project's scope encompasses the development and implementation of an internal solution designed to automatically scan multifamily (MF) appraisal comments for prohibited language. Leveraging a two-stage artificial intelligence model, this solution will closely mirror the functionality of the existing APLD model used by the Single Family Collateral Risk team. The scope includes parsing appraisal reports into sentences, detecting biased phrases based on a predefined keyword list, and employing a text classification model to minimize false positives and negatives in the appraisal review process.
2. Model Usage
The proposed model operates on a two-stage principle to ensure thorough and accurate detection of prohibited language within multifamily appraisal comments. The first stage involves receiving a list of biased categories from the user, alongside the appraisal files. The model then parses these reports into sentences, utilizing a keyword list provided by the Fair Lending team to identify phrases closely resembling prohibited language related to the input categories. This initial stage allows users to maintain control over the categories deemed prohibited, ensuring that the model's focus aligns with current regulatory standards and organizational policies.
In the second stage, the model employs a sophisticated text classification algorithm to sift through the results from the first stage, aiming to accurately distinguish between true and false positives. This dual-stage approach not only enhances the precision of the detection process but also significantly reduces the likelihood of overlooking genuine instances of biased language or erroneously flagging innocuous content.
3. Scope of Model Changes
The introduction of the MFPLD model signifies a significant advancement in Fannie Mae's approach to managing collateral risk and ensuring fair lending practices. While the model inherits its foundational principles from the Single Family's APLD model, it incorporates several key enhancements to address the unique challenges presented by multifamily appraisal comments. The most notable change lies in the adaptation and expansion of the keyword list, tailored to reflect the nuanced language commonly found in multifamily appraisals. Additionally, the inclusion of a machine learning model in the second stage represents a strategic enhancement aimed at improving the accuracy of bias detection by reducing false positives and negatives. These modifications are expected to augment the model's effectiveness, enabling a more streamlined, accurate, and efficient review process.
4. Model Overview
The MFPLD model represents a cutting-edge solution designed to automate the detection of prohibited language in multifamily appraisal comments. At its core, the model is structured around a two-stage artificial intelligence system that mirrors the proven efficacy of the APLD model used within the Single Family sector. The first stage focuses on parsing appraisal reports into sentences and identifying phrases that closely match a predefined list of biased keywords. This approach allows for precise detection of potentially prohibited language, with a significant emphasis on user-defined categories.
The second stage introduces an advanced text classification model, which meticulously reviews the findings from the first stage to discern between true and false positives. This layered approach ensures a high degree of accuracy in identifying biased language, effectively minimizing the risks associated with manual review processes. Through its innovative design and strategic implementation, the MFPLD model is poised to revolutionize Fannie Mae's appraisal review process, offering a scalable, efficient, and reliable solution for upholding the highest standards of fairness and integrity in property valuation.
---------------------------------------------------------------------------------------------------------------------------------------------------------
1. Background, Purpose, and Scope
The underwriting process at Fannie Mae involves the analysis of thousands of documents annually, including recommendation memos, sponsor credit reviews, and narratives from sponsors and lenders. These documents play a crucial role in assessing the value of multifamily properties, which is essential for determining collateral worth. However, not all information contained within these reports contributes to an accurate property valuation. Subjective opinions or biases based on protected characteristics, which are prohibited under fair lending laws and regulations, can inadvertently influence these assessments.
Since 2021, the MF Lender Risk Management team at Fannie Mae has been tasked with manually reviewing these documents to identify and flag language that violates the Fannie Mae Selling Guide or introduces subjective bias that could affect property valuation. Identified violations prompt notifications to appraisers about the use of such language. This manual review process, while necessary, is both time-consuming and costly.
The purpose of this initiative is to develop an automated solution capable of detecting prohibited language within MF appraisal comments efficiently and reliably. The scope includes the automation of bias detection in appraisal comments, leveraging AI technology to streamline the process previously managed through manual review. This move aims to significantly reduce the workload and associated costs of manual bias detection in appraisal documents, enhancing efficiency and accuracy in Fannie Mae's underwriting process.
2. Model Usage
The proposed solution is a two-stage AI model, designed to automate the detection of prohibited language within multifamily (MF) appraisal comments. The first stage involves receiving a list of biased categories and the appraisal files from the user. The model then parses these appraisal reports into sentences and employs a keyword list, pre-defined by the Fair Lending team, to identify sentences that contain language similar to the biased phrases related to the input categories. This process ensures that detection is aligned with the specific prohibitions deined by the individuals overseeing these categories.

The second stage of the model applies a text classification technique to sift through the results of the first stage, aiming to identify false positives. This two-pronged approach mirrors the APLD model developed for the Collateral Risk team, emphasizing the reduction of both false positives and false negatives in the detection process. By adding a machine learning (ML) model in the second stage, the solution endeavors to enhance the reliability and efficiency of detecting biased language in appraisal comments, thereby reducing the necessity for manual review.
3. Scope of Model Changes
The MFPLD model represents an evolution from the previously developed APLD model, specifically tailored for the appraisal comments of multifamily properties. This new model introduces several critical enhancements to accommodate the unique requirements of MF appraisal document review. Firstly, it incorporates a more extensive and detailed pre-defined keyword list provided by the Fair Lending team, ensuring comprehensive coverage of biased language across various categories.
Furthermore, the addition of a machine learning model in the second stage marks a significant improvement aimed at refining the detection process. This ML model is tasked with reducing the rates of false positives and false negatives, addressing one of the main challenges in automated language detection. By focusing on the accuracy and reliability of the detection mechanism, the MFPLD model aims to significantly diminish the manual effort required in reviewing appraisal documents for potential bias, streamlining the process for Fannie Mae's MF Lender Risk Management team.
4. Model Overview
The MFPLD model is a sophisticated AI solution designed to automate the detection of prohibited language within appraisal comments for multifamily properties. It operates in two distinct stages to ensure high accuracy and efficiency. In the first stage, the model receives user-defined biased categories along with the appraisal files. It then parses these files into sentences and employs a pre-defined keyword list to detect sentences that may contain biased language related to the specified categories.
====================================================================================================
1.1 Background, Purpose, and Scope
Background: Fannie Mae processes thousands of documents annually as part of its multifamily property loan underwriting procedures. These documents, including recommendation memos and sponsor credit reviews, play a critical role in evaluating the collateral worth of properties. A recurring challenge has been the presence of biased language within these reports, which can affect the accuracy of property valuations. This biased language, often subtle and rooted in subjective opinions or protected characteristics, contravenes fair lending laws and regulations.
Purpose: The purpose of developing the MFPLD (Multifamily Prohibited Language Detector) model is to automate the detection of prohibited language within appraisal comments, thereby enhancing the accuracy and fairness of property valuations. This initiative stems from the need to improve efficiency and reduce the manual labor involved in reviewing appraisal reports for biased language, a process that is both time-consuming and costly.
Scope: The model is designed to automatically scan and detect biased language in multifamily appraisal comments, leveraging a two-stage AI solution for this purpose. It is an internal solution closely related to the APLD model developed for the Collateral Risk team, aimed at significantly reducing the workload associated with manual reviews of appraisal reports for potential bias. The MFPLD model applies specifically to the multifamily property loan underwriting process at Fannie Mae, addressing a previously unaddressed problem of automated bias detection in appraisal reports.
Model Development: This is a new model, developed to address the challenges of manually detecting biased language in appraisal reports—a process previously unautomated. The MFPLD model represents an innovative approach to enhancing the accuracy and fairness of property valuations at Fannie Mae.
1.2 Model Usage
Application: The MFPLD model is used within the loan underwriting process, specifically in assessing the collateral worth of multifamily properties. Its primary function is to automate the detection of prohibited language in appraisal comments, contributing to more accurate and unbiased property valuations.
Product Types and Segmentation: The model is applicable to multifamily property appraisals, serving as a critical tool in the loan underwriting process for these properties.
Expected Results: By automating the detection of biased language, the model aims to improve the efficiency, accuracy, and fairness of property valuations. This contributes to a more equitable lending process, in line with fair lending laws and regulations.
Known and Planned Usages: Currently, the MFPLD model is used to scan and detect prohibited language in multifamily appraisal comments. Future enhancements and broader applications within Fannie Mae's loan underwriting processes are anticipated as the model's capabilities evolve.
1.3 Scope of Model Changes
Major Changes: Given that the MFPLD model is a new development, this section would typically outline any significant changes from previous versions or related models. In the case of the MFPLD, it can be noted that it introduces a two-stage AI solution for detecting prohibited language, an advancement over manual review processes previously employed.
Minor Changes: Any minor changes or enhancements made during the development phase would be detailed in Appendix E, as per the document structure.
1.4 Model Overview
Reasons for Creation: The MFPLD model was created to address the inefficiencies and inaccuracies associated with manually reviewing appraisal reports for biased language. By automating this process, Fannie Mae aims to enhance the fairness and accuracy of property valuations for multifamily properties.
Enhancements in Modeling Methodology: The model introduces a two-stage AI solution, incorporating both keyword detection and text classification to accurately identify prohibited language in appraisal comments. This methodology represents a significant enhancement over previous manual processes.
Improvements in Model Performance: The automation of biased language detection is expected to significantly reduce the time and cost associated with manual reviews, while also improving the accuracy and fairness of property valuations.
Upstream and Downstream Models: The MFPLD model works in conjunction with other models in the loan underwriting process, such as the APLD model for the Collateral Risk team. A depiction of model interdependencies, including data flows, would illustrate how the MFPLD model integrates within Fannie Mae's broader risk management framework.
Component Model Information Systems: The development and operation of the MFPLD model involve various programs, software, and technologies, including AI and machine learning platforms, which enable the parsing of appraisal reports and the detection of biased language.
This content provides a structured outline for each of the segments based on the executive summary provided.
The second stage involves a machine learning (ML) model that scrutinizes the results from the first stage, identifying and filtering out false positives. This dual-stage approach enhances the model's capability to accurately detect prohibited language, thereby significantly reducing the manual review workload.
The development of the MFPLD model is a testament to Fannie Mae's commitment to leveraging technology for improving efficiency and accuracy in the underwriting process. By automating the detection of biased language in appraisal documents, the MFPLD model represents a critical step forward in streamlining operations and upholding the integrity of property valuations under fair lending laws and regulations.
--------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------
The MFPLD (Multifamily Prohibited Language Detector) model is designed to streamline the process of identifying biased language within appraisal reports, a crucial step in ensuring fair and accurate property evaluations. At the outset, users input specific categories of bias they wish to detect, along with the appraisal documents to be analyzed. Following this initial setup, the model meticulously segments the reports into individual sentences. It then employs an advanced algorithm to scrutinize each sentence against a comprehensive list of biased phrases. This list is meticulously curated by the Fair Lending team, ensuring it reflects the most current understanding of language that could unfairly influence property valuations. This approach empowers users with the authority to define and target specific prohibited language categories, tailoring the detection process to meet stringent fairness criteria.
In the subsequent phase of analysis, the model leverages a sophisticated text classification system to sift through the preliminary findings. This is where the model's intelligence shines, as it diligently distinguishes between true instances of biased language and those sentences falsely flagged in the initial screening. This dual-layered approach, inspired by the proven efficacy of the APLD model used by the Collateral Risk team, incorporates machine learning techniques to refine its accuracy further. By minimizing the rates of both false positives (innocuous sentences incorrectly flagged as biased) and false negatives (biased sentences that go undetected), the model significantly enhances the efficiency and reliability of the detection process.
This two-pronged strategy represents a potent solution to the traditionally labor-intensive task of manually reviewing appraisal reports for bias. By automating the detection of prohibited language with such precision and control, the MFPLD model promises to drastically reduce the human effort required in these analyses. Consequently, this not only speeds up the appraisal review process but also elevates the standard of fairness in property valuations, aligning closely with the overarching objectives of fair lending practices.
111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111
Thank you for providing the detailed conceptual framework and model structure for the Multifamily Prohibited Language Detection (MFPLD) model. This is a comprehensive overview that covers the key components, mathematical specifications, and literature survey supporting the model's design. Let me summarize the key points:

Biased Language Categories and Mapping Dictionary:
The model uses 10 high-level biased language categories (e.g., sexual orientation, gender, race/ethnicity) and a mapping dictionary of 620 corresponding biased phrases.
Sentence Parsing and Embedding:
The model parses documents into sentences and generates 256-dimensional sentence embeddings using the sentence-transformers-all-miniLM-l6v2 model.
The model also generates embeddings for the 620 biased phrases.
Similarity Calculation:
The model calculates the cosine similarity between each sentence and the 620 biased phrases.
The maximum similarity score for each of the 10 high-level biased language categories is reported.
Bias Detection:
If any of the 10 similarity scores for a sentence exceeds a global similarity threshold (0.50 in this case), the model flags the sentence as containing biased language.
The conceptual framework is based on the premise that biased language in mortgage appraisal and related documents can influence property valuation, leading to unfair and inequitable outcomes. The two-stage approach, combining keyword-based detection and text classification, is a novel and unique solution compared to alternative methodologies like rule-based, supervised machine learning, and unsupervised anomaly detection approaches.The mathematical specifications and literature survey provided demonstrate the solid theoretical and empirical foundations of the MFPLD model. The use of sentence embeddings, similarity calculations, and a global threshold for bias detection are well-supported by the referenced research.
Overall, this is a comprehensive and well-designed framework for detecting prohibited language in mortgage-related documents, with the potential to improve fairness and equity in property valuation. The hybrid approach leverages the strengths of both rule-based and machine learning techniques to create a robust and effective solution.
-------------------------------------------------------------------------------------------------------------------------------------------------------------
Conceptual Framework and Model Structure

The Multifamily Prohibited Language Detection (MFPLD) model is a two-stage AI-based solution designed to automatically detect prohibited language in mortgage appraisal and other related documents that may affect property valuation. The model consists of the following key components:

Biased Language Categories and Mapping Dictionary
The model receives a list of 10 high-level biased language categories, such as sexual orientation, gender, location, desirability, disability, income-level, familial status, race/nationality/ethnicity, age, and demographics.
The model also receives a mapping dictionary that maps each high-level category to 620 corresponding biased phrases, such as "poor family," "low-income community," and so on.
Sentence Parsing and Embedding
The model receives appraisal and other related documents and parses them into sentences.
The model uses the sentence-transformers-all-miniLM-l6v2 model to generate 256-dimensional embedding vectors for each sentence and for the 620 biased phrases.
Similarity Calculation
After mean-pooling and normalizing the embedding vectors, the model calculates the cosine similarity between each sentence and the 620 biased phrases from the 10 high-level categories.
The model then reports the 10 high-level category similarity scores by finding the maximum similarity score from the biased phrases related to each high-level category.
Bias Detection
If any of the 10 similarity scores for a sentence is higher than a global similarity threshold (set to 0.50 in this case), the model adds a binary variable to the report indicating that the sentence contains biased language.
The conceptual framework of the MFPLD model is based on the premise that subjective opinions or biased language in mortgage appraisal and related documents can influence property valuation, potentially leading to unfair and inequitable outcomes. By automating the detection of such prohibited language, the model aims to reduce the workload for manual review and ensure fair and equitable collateral valuation.

Mathematical Specifications

The MFPLD model leverages several mathematical and statistical techniques to achieve its objectives:

Sentence Embedding:
The model uses the sentence-transformers-all-miniLM-l6v2 model to generate 256-dimensional embedding vectors for each sentence and the 620 biased phrases.
The embedding vectors are mean-pooled and normalized to obtain the final sentence and biased phrase representations.
Similarity Calculation:
The model calculates the cosine similarity between each sentence and the 620 biased phrases using the formula:

Copy code
similarity = dot(sentence_embedding, biased_phrase_embedding) / (norm(sentence_embedding) * norm(biased_phrase_embedding))
The maximum similarity score for each of the 10 high-level biased language categories is then reported.
Bias Detection:
The model compares the maximum similarity scores for each sentence against a global similarity threshold (set to 0.50 in this case).
If any of the similarity scores exceed the threshold, the model adds a binary variable to the report indicating the presence of biased language in the sentence.
Literature Survey

The conceptual framework and modeling techniques used in the MFPLD model are supported by various academic and industry publications:

Sentence Embedding and Similarity Calculation:
Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) (pp. 3982-3992). Link
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems (pp. 5998-6008). Link
Bias Detection in Language Models:
Bolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., & Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? Debiasing word embeddings. In Advances in neural information processing systems (pp. 4349-4357). Link
Zhao, J., Wang, T., Yatskar, M., Ordonez, V., & Chang, K. W. (2017). Men also like shopping: Reducing gender bias amplification using corpus-level constraints. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (pp. 2979-2989). Link
Bias in Mortgage Appraisals:
Howell, J., & Korver-Glenn, E. (2020). The Increasing Effect of Neighborhood Racial Composition on Housing Values, 1980–2015. Social Problems, 68(4), 1025-1044. Link
Perry, A. M., Rothwell, J., & Harshbarger, D. (2018). The devaluation of assets in Black neighborhoods: The case of residential property. Brookings Institution. Link
Evaluation of Alternative Methodologies

The MFPLD model's two-stage approach, combining keyword-based detection and text classification, is a novel and unique approach to the problem of detecting prohibited language in mortgage appraisal and related documents.

Alternative methodologies that were considered and their strengths and weaknesses include:

Rule-based Approach:
Strengths: Simple to implement, easy to understand, and provides transparent decision-making.
Weaknesses: Requires extensive manual curation of rules, may miss subtle or context-dependent biases, and may have high false positive rates.
Supervised Machine Learning Approach:
Strengths: Can learn complex patterns from data, potentially more accurate than rule-based approaches.
Weaknesses: Requires a large annotated dataset, which can be time-consuming and expensive to obtain, and may struggle with out-of-sample data.
Unsupervised Anomaly Detection Approach:
Strengths: Can identify unusual or anomalous language without prior labeling, may uncover previously unknown biases.
Weaknesses: May have high false positive rates, difficult to interpret and explain the results.
The two-stage approach of the MFPLD model, combining the strengths of keyword-based detection and text classification, is designed to address the limitations of these alternative methodologies. The keyword-based detection provides a transparent and controllable mechanism for identifying prohibited language, while the text classification model helps to reduce false positives and false negatives. This hybrid approach leverages the benefits of both rule-based and machine learning techniques, resulting in a more robust and effective solution for detecting prohibited language in mortgage appraisal and related documents.
-------------------------------------------------------------------------------------------------------------------------------------------------------------
The development of a model aimed at identifying and analyzing subjective opinions or biased language within mortgage appraisal documents and related content is a critical step towards ensuring fairness and objectivity in property valuation processes. The model described here uses a sophisticated blend of natural language processing (NLP) techniques and sentence embedding technologies to detect biased language across ten high-level categories. This white paper outlines the conceptual framework, mathematical specifications, and theoretical background of the model, evaluates alternative methodologies, and discusses the strengths and weaknesses of various approaches.

Conceptual Framework and Model Structure
The model is designed to process appraisal and related documents by parsing the text into sentences and analyzing these sentences for biased language corresponding to ten predetermined categories (e.g., sexual orientation, gender, location, desirability). It achieves this through the use of the sentence-transformers-all-miniLM-l6v2 for embedding both the biased phrases and sentences from the documents. By calculating the cosine similarity between sentence embeddings and pre-defined biased phrase embeddings, the model identifies and quantifies the level of bias in each sentence.

Model Structure:
Input Processing: Receives documents and a mapping dictionary linking high-level bias categories to specific biased phrases (620 in total).
Sentence Embedding: Uses sentence-transformers-all-miniLM-l6v2 for embedding sentences and biased phrases, employing mean-pooling and normalization to process embedding vectors.
Cosine Similarity Calculation: Computes the cosine similarity between sentence embeddings and biased phrase embeddings.
Bias Detection: Assigns similarity scores to sentences based on their maximum similarity to biased phrases within each of the ten categories. Sentences with scores above a 0.50 threshold are flagged as biased.
Mathematical Specifications
The core mathematical operation in this model is the calculation of cosine similarity between the embedding vectors of sentences and biased phrases. 
The cosine similarity is defined as:

B are the embedding vectors of a sentence and a biased phrase, respectively. Mean-pooling is applied to embeddings to obtain a fixed-length vector for each sentence or phrase, followed by normalization to ensure comparability.

Literature Survey and Theoretical Background
The theoretical foundation of this model is rooted in the domain of natural language processing and machine learning, particularly in the use of embeddings for semantic analysis and bias detection. Sentence embeddings, as popularized by models like BERT and its variants (such as miniLM), have shown considerable success in capturing the contextual meanings of sentences (Devlin et al., 2018; Wang et al., 2020).

The application of cosine similarity measures to compare embeddings is a well-established technique for semantic similarity assessment, offering a straightforward yet powerful means of identifying related concepts within texts (Mikolov et al., 2013).

Evaluation of Alternative Methodologies
Several alternative methodologies could be considered for this task, including rule-based systems, classic machine learning classifiers (e.g., SVMs, Random Forests), and other forms of deep learning models (e.g., RNNs, CNNs).

Strengths and Weaknesses:
Rule-Based Systems: While interpretable, they lack the flexibility and scalability of ML approaches, often missing nuanced expressions of bias.
Classic Machine Learning: These models can be effective but require extensive feature engineering and may not capture the contextual nuances as effectively as embedding-based methods.
Deep Learning Models (RNNs, CNNs): Though powerful for text analysis, they may require more data and computational resources than embedding-based approaches.
Rationale for Selected Methodology
The decision to employ sentence embeddings, particularly using the miniLM model for embedding generation, was driven by the model's ability to efficiently capture deep semantic meanings of text. The use of cosine similarity provides a straightforward and computationally efficient mechanism to assess the degree of bias within sentences. This approach balances performance with computational efficiency, making it particularly suitable for processing large volumes of text without sacrificing accuracy.

Conclusion
The described model represents a novel approach to detecting biased language within mortgage appraisal documents, leveraging the latest advancements in NLP and embedding technologies. Its design and methodology are grounded in sound mathematical principles and supported by existing literature, offering a balanced approach to bias detection that is both efficient and effective.
Appendix D: References
Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv.
Wang, Y., Wang, W., Xu, J., & Shen, J. (2020). MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. arXiv.
Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient Estimation of Word Representations in Vector
