import nltk
from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters
import re

nltk.download('punkt')

def advanced_sentence_tokenizer(text):
    punkt_param = PunktParameters()
    expanded_abbreviation_list = [
        'e.g', 'i.e', 'etc', 'mr', 'mrs', 'dr', 'ph.d', 'prof', 'inc', 'ltd',
        'co', 'jr', 'sr', 'vs', 'dept', 'univ', 'assn', 'bros', 'ph', 'md', 
        'do', 'dvm', 'od', 'dds', 'llc', 'llp', 'corp', 'jan', 'feb', 'mar', 
        'apr', 'jun', 'jul', 'aug', 'sep', 'sept', 'oct', 'nov', 'dec', 
        'mon', 'tue', 'tues', 'wed', 'thu', 'thurs', 'fri', 'sat', 'sun', 
        'no', 'vol', 'ed', 'pp', 'est', 'fig', 'eq', 'ex', 'cf', 'ref', 
        'refs', 'ch', 'sec', 'viz', 'al', 'ave', 'blvd', 'st', 'rd', 'mt',
        # Add more abbreviations as needed
    ]
    punkt_param.abbrev_types = set(expanded_abbreviation_list)
    tokenizer = PunktSentenceTokenizer(punkt_param)
    
    # Replace curly quotes with straight quotes
    text = text.replace('“', '"').replace('”', '"').replace("‘", "'").replace("’", "'")
    
    # Remove special characters (optional, based on need)
    # Be careful with this step as it may remove characters that are meaningful in certain contexts
    # For example, keeping periods, commas, question marks, etc., is important for sentence tokenization
    # text = re.sub(r"[^a-zA-Z0-9.,!?\'\"]", " ", text)
    
    # Additional preprocessing to handle specific cases, e.g., protecting decimal points in numbers
    text = re.sub(r'(\d)\.(\d)', r'\1DECIMAL\2', text)
    text = re.sub(r'\n+', '. ', text)  # Replace newlines with periods to separate statements properly
    
    sentences = tokenizer.tokenize(text)
    
    # Post-tokenization: clean up any placeholders or special handling
    sentences = [sent.replace('DECIMAL', '.') for sent in sentences]
    
    return sentences










------------------------------------------------------------------------------------------------------------------------------------

focusing on the rates of false negatives relative to false positives. Remember, this visualization emphasizes the cost of false negatives (missed detections) versus false positives (false alarms), which might be particularly relevant in contexts where missing a positive case has serious implications, like in medical diagnostics or fraud detection.



fpr, tpr, thresholds = roc_curve(y_true, y_scores)
roc_auc = auc(fpr, tpr)

# Plotting the ROC curve with emphasis on FNR
plt.figure()
plt.plot(fpr, 1 - tpr, label='FNR vs. FPR (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [1, 0], linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('False Negative Rate (FNR)')
plt.title('FNR vs. FPR Curve')
plt.legend(loc="lower left")
plt.show()

================================================================================



import numpy as np
from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

# Sample predicted scores and true labels for Model 1 and Model 2
y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])  # Example true labels
y_scores_model1 = np.array([0.9, 0.1, 0.8, 0.72, 0.3, 0.67, 0.24, 0.45, 0.85, 0.15])  # Predicted scores from Model 1
y_scores_model2 = np.array([0.85, 0.2, 0.75, 0.8, 0.25, 0.7, 0.3, 0.4, 0.9, 0.05])  # Predicted scores from Model 2

# Function to plot ROC curve
def plot_roc_curve(y_true, y_scores, title):
    fpr, tpr, thresholds = roc_curve(y_true, y_scores)
    roc_auc = auc(fpr, tpr)

    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(title)
    plt.legend(loc="lower right")
    plt.show()

# Plot ROC curve for each model
plot_roc_curve(y_true, y_scores_model1, 'ROC Curve for Model 1')
plot_roc_curve(y_true, y_scores_model2, 'ROC Curve for Model 2')



def plot_combined_roc_curve(y_true, y_scores_model1, y_scores_model2):
    fpr1, tpr1, thresholds1 = roc_curve(y_true, y_scores_model1)
    roc_auc1 = auc(fpr1, tpr1)

    fpr2, tpr2, thresholds2 = roc_curve(y_true, y_scores_model2)
    roc_auc2 = auc(fpr2, tpr2)

    plt.figure()
    plt.plot(fpr1, tpr1, color='darkorange', lw=2, label='Model 1 (area = %0.2f)' % roc_auc1)
    plt.plot(fpr2, tpr2, color='green', lw=2, label='Model 2 (area = %0.2f)' % roc_auc2)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Combined ROC Curves')
    plt.legend(loc="lower right")
    plt.show()

# Plot combined ROC curve
plot_combined_roc_curve(y_true, y_scores_model1, y_scores_model2)






















































def find_similar_words(input_text, top_n=10):
    # Process the input text with the spaCy transformer model
    doc = trf_model(input_text)
    
    # Initialize a list to store similar words and phrases
    similar_words = []
    
    # Iterate through tokens in the processed text
    for token in doc:
        # Check if the token has vector representation (e.g., excluding stopwords)
        if token.has_vector:
            # Calculate the similarity score between the input token and all tokens in the vocabulary
            similarity_scores = [token.similarity(other_token) for other_token in doc]
            
            # Sort tokens by similarity score in descending order
            sorted_tokens = sorted(enumerate(similarity_scores), key=lambda x: x[1], reverse=True)
            
            # Extract the top N similar tokens (excluding the input token itself)
            similar_tokens = [doc[i[0]].text for i in sorted_tokens if i[0] != token.i][:top_n]
            
            # Add the similar tokens to the list
            similar_words.extend(similar_tokens)
    
    # Remove duplicates from the list
    similar_words = list(set(similar_words))
    
    return similar_words

-=-===================================----------------55555555555555555555555555555555555555555
def compute_metrics(TP, FN, FP, TN):
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    precision = TP / (TP + FP) if TP + FP != 0 else 0
    recall = TP / (TP + FN) if TP + FN != 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0
    balanced_accuracy = 0.5 * ((TP / (TP + FN) if TP + FN != 0 else 0) + (TN / (TN + FP) if TN + FP != 0 else 0))
    mcc_denom = ((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) ** 0.5
    mcc = (TP * TN - FP * FN) / mcc_denom if mcc_denom != 0 else 0
    fbeta = (1 + 2**2) * (precision * recall) / ((2**2 * precision) + recall) if precision + recall != 0 else 0
    
    return {
        "Accuracy": accuracy,
        "Precision": precision,
        "Recall": recall,
        "F1 Score": f1_score,
        "Balanced Accuracy": balanced_accuracy,
        "MCC": mcc,
        "F-beta (beta=2)": fbeta
    }

metrics_C = compute_metrics(9, 0, 24, 158)

print("Metrics for Model C:")
for metric, value in metrics_C.items():
    print(f"{metric}: {value:.4f}")
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Get input values
TP = int(input("Enter the number of True Positives (TP): "))
TN = int(input("Enter the number of True Negatives (TN): "))
FP = int(input("Enter the number of False Positives (FP): "))
FN = int(input("Enter the number of False Negatives (FN): "))

# Calculate metrics
accuracy = (TP + TN) / (TP + TN + FP + FN)
recall = TP / (TP + FN) if TP + FN != 0 else 0
precision = TP / (TP + FP) if TP + FP != 0 else 0
f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0
total_instances = TP + TN + FP + FN

# Visualize the confusion matrix
confusion_matrix = np.array([[TP, FP], [FN, TN]])
ax = sns.heatmap(confusion_matrix, annot=True, fmt='g', cmap='Blues', cbar=False)
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.xaxis.set_ticklabels(['Positive', 'Negative'])
ax.yaxis.set_ticklabels(['Positive', 'Negative'])
plt.show()

# Display metrics
print(f"\n\n{'Accuracy:':<15} {accuracy:.4f}")
print(f"{'Recall:':<15} {recall:.4f}")
print(f"{'Precision:':<15} {precision:.4f}")
print(f"{'F1-score:':<15} {f1_score:.4f}")
print(f"{'Total instances:':<15} {total_instances}")

=------------------------------===================================-------------------------------------5678090
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load and prepare the multi-class dataset
data_multiclass = pd.read_csv('your_multiclass_data.csv')  # Adjust the file path
X_multi = data_multiclass.drop('label', axis=1)
y_multi = data_multiclass['label']

# Load and prepare the binary classification dataset
data_binary = pd.read_csv('your_binary_data.csv')  # Adjust the file path
X_binary = data_binary.drop('label', axis=1)
y_binary = data_binary['label']

# Standardize the features
scaler = StandardScaler()
X_multi_scaled = scaler.fit_transform(X_multi)
X_binary_scaled = scaler.fit_transform(X_binary)

# Split the multi-class data
X_multi_train, X_multi_temp, y_multi_train, y_multi_temp = train_test_split(X_multi_scaled, y_multi, test_size=0.4, stratify=y_multi, random_state=42)
X_multi_val, X_multi_test, y_multi_val, y_multi_test = train_test_split(X_multi_temp, y_multi_temp, test_size=0.5, stratify=y_multi_temp, random_state=42)

# Split the binary classification data
X_binary_train, X_binary_temp, y_binary_train, y_binary_temp = train_test_split(X_binary_scaled, y_binary, test_size=0.4, stratify=y_binary, random_state=42)
X_binary_val, X_binary_test, y_binary_val, y_binary_test = train_test_split(X_binary_temp, y_binary_temp, test_size=0.5, stratify=y_binary_temp, random_state=42)


from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier

# Initialize multi-class models
xgb_multi_model = XGBClassifier()  # Add parameters as needed
rf_multi_model = RandomForestClassifier()  # Add parameters as needed

# Train multi-class models
xgb_multi_model.fit(X_multi_train, y_multi_train)
rf_multi_model.fit(X_multi_train, y_multi_train)


from sklearn.model_selection import GridSearchCV

# Example: Grid search for XGBoost multi-class model
param_grid_multi_xgb = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 300]
}

grid_search_multi_xgb = GridSearchCV(xgb_multi_model, param_grid_multi_xgb, cv=3, scoring='accuracy', verbose=2)
grid_search_multi_xgb.fit(X_multi_val, y_multi_val)

# Update multi-class XGBoost model with the best parameters
xgb_multi_model = grid_search_multi_xgb.best_estimator_

# Similar tuning for the RandomForest multi-class model...


# Generate multi-class predictions as a feature for the binary classification dataset
multi_predictions_train = xgb_multi_model.predict(X_multi_train)  # You can also try using rf_multi_model
multi_predictions_val = xgb_multi_model.predict(X_multi_val)
multi_predictions_test = xgb_multi_model.predict(X_multi_test)

--------------------------=============---------------------------------------=
from sklearn.preprocessing import OneHotEncoder

# Assuming your categorical variable is in a column named 'category'

# Initialize OneHotEncoder with known categories
encoder = OneHotEncoder(categories=[range(0, 9)], sparse=False)

# Fit the encoder on the entire data (if available) or just use the known categories
# If you have a dataset that contains all categories, use it to fit. Otherwise, the line below is fine.
encoder.fit([[0], [1], [2], [3], [4], [5], [6], [7], [8]])

# Transform the data in train, validation, and test sets
train_encoded = encoder.transform(train['category'].values.reshape(-1, 1))
val_encoded = encoder.transform(val['category'].values.reshape(-1, 1))
test_encoded = encoder.transform(test['category'].values.reshape(-1, 1))

-================================---------------------------


# Add multi-class predictions as a feature to the binary dataset
X_binary_train_with_multi = np.column_stack((X_binary_train, multi_predictions_train))
X_binary_val_with_multi = np.column_stack((X_binary_val, multi_predictions_val))
X_binary_test_with_multi = np.column_stack((X_binary_test, multi_predictions_test))



# Initialize binary classification models
xgb_binary_model = XGBClassifier(scale_pos_weight = sum(y_binary_train == 0) / sum(y_binary_train == 1)) 
rf_binary_model = RandomForestClassifier(class_weight='balanced')

# Train binary models using the modified dataset
xgb_binary_model.fit(X_binary_train_with_multi, y_binary_train)
rf_binary_model.fit(X_binary_train_with_multi, y_binary_train)

Step 6: Hyperparameter Tuning for Binary Models



from sklearn.metrics import classification_report, accuracy_score

# Predict
































-========================----------------43=3444444444444444------------------------------==============================
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load data
data = pd.read_csv('your_data.csv')  # Adjust the file path

# Separate features and target
X = data.drop('label', axis=1)
y = data['label']

# Standardize the features (optional but recommended)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split the data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.4, stratify=y, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier

# Initialize models
xgb_model = XGBClassifier(scale_pos_weight = sum(y_train == 0) / sum(y_train == 1))  # Adjust for imbalance
rf_model = RandomForestClassifier(class_weight='balanced')  # Adjust for imbalance

# Train models
xgb_model.fit(X_train, y_train)
rf_model.fit(X_train, y_train)


from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 300]
}

# Grid search
grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='roc_auc', verbose=2)
grid_search.fit(X_val, y_val)

# Update model with best parameters
xgb_model = grid_search.best_estimator_



from sklearn.model_selection import GridSearchCV

# Parameter grid for Random Forest
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create GridSearchCV object for Random Forest
grid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=3, scoring='roc_auc', verbose=2)
grid_search_rf.fit(X_val, y_val)

# Update Random Forest model with the best parameters
rf_model = grid_search_rf.best_estimator_


from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [100, 200, 300]
}

# Grid search
grid_search = GridSearchCV(xgb_model, param_grid, cv=3, scoring='roc_auc', verbose=2)
grid_search.fit(X_val, y_val)

# Update model with best parameters
xgb_model = grid_search.best_estimator_


from sklearn.ensemble import VotingClassifier

# Create a voting classifier
voting_clf = VotingClassifier(
    estimators=[('xgb', xgb_model), ('rf', rf_model)],
    voting='soft'
)

# Fit the voting classifier
voting_clf.fit(X_train, y_train)


from sklearn.metrics import classification_report, accuracy_score

# Predictions
y_pred = voting_clf.predict(X_test)

# Evaluation
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))








































-===================================44444444444444444444444444444444444444444444444444444444444
import pandas as pd

# Define the function to read an .xlsm file and read all text-related data from all sheet tabs ignoring null, non-textual, and numeric values.
def read_xlsm_text_advanced(file_path):
    # Load the Excel file
    xl = pd.ExcelFile(file_path)

    # Create an empty list to store text data
    text_data = []

    # Define a function to check if a cell contains text data (ignoring purely numeric and mixed alphanumeric strings)
    def is_text(cell):
        if isinstance(cell, str):
            # Check if all characters in the string are alphabetic or whitespace
            return all(char.isalpha() or char.isspace() for char in cell)
        return False

    # Iterate through each sheet in the workbook
    for sheet in xl.sheet_names:
        # Read the sheet into a pandas DataFrame
        df = xl.parse(sheet)

        # Iterate through the DataFrame to extract text values
        for column in df.columns:
            for item in df[column]:
                # Check if the item is text-related data
                if pd.notnull(item) and is_text(item):
                    # Add the text to the list, stripping leading/trailing whitespace
                    text_data.append(item.strip())

    # Return the list of text data
    return text_data

# Example usage:
# text_from_xlsm = read_xlsm_text_advanced('path_to_file.xlsm')
# This is commented out to prevent an error since there is no file to read in this environment.

-------------------------------======================================5555555555555555555555555555

import spacy

nlp = spacy.load("en_core_web_md")  # Load the medium-sized English model

def find_similar(keyword, threshold=0.89):
    """Find words similar to keyword using spaCy word vectors"""
    
    doc = nlp(keyword)
    query_vector = doc.vector

    similar_words = []
    for word in nlp.vocab:
        if word.has_vector:  # Check if the word has a vector representation
            similarity = query_vector.dot(word.vector)  # Calculate cosine similarity
            if similarity > threshold:
                similar_words.append((word.text, similarity))
    
    similar_words = sorted(similar_words, key=lambda x: x[1], reverse=True)

    print(f"Similar words to {keyword} with similarity > {threshold}:")
    for word, score in similar_words:
        print(f"- {word} ({score:.3f})")

# Example usage:
keyword = "house"
find_similar(keyword)


-------------------------------------------------------------
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel

# Constants
MAX_LEN = 512
EMBEDDING_DIM = 8000
NUM_ASPECTS = 9
DROPOUT_RATE = 0.5

# Custom attention layer
class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1),
                                 initializer="normal")
        self.b = self.add_weight(name="att_bias", shape=(input_shape[1], 1),
                                 initializer="zeros")
        super(AttentionLayer, self).build(input_shape)

    def call(self, x):
        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)
        a = tf.keras.backend.softmax(e, axis=1)
        output = x * a
        return tf.keras.backend.sum(output, axis=1)

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[-1])

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Input Layer
input_text = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name="input_text")

# BERT for contextual embeddings
bert_model = TFBertModel.from_pretrained('bert-base-uncased')
bert_output = bert_model(input_text)[0]

# Attention layer
attention_output = AttentionLayer()(bert_output)

# Dropout layer after attention
attention_output = tf.keras.layers.Dropout(DROPOUT_RATE)(attention_output)

# Custom layer to concatenate attention outputs to desired dimension
concat_layer = tf.keras.layers.Dense(EMBEDDING_DIM, activation='relu')(attention_output)
concat_layer = tf.keras.layers.Dropout(DROPOUT_RATE)(concat_layer)

# ... [Rest of the model remains the same]

# Print model summary
model.summary()



-=--=========================
# Compile the model with F1-Score as a metric
f1_score = tfa.metrics.F1Score(num_classes=3, average='macro')  # For multi-class tasks
binary_f1_score = tfa.metrics.F1Score(num_classes=1, threshold=0.5)  # For binary tasks

model.compile(optimizer='adam', 
              loss=['categorical_crossentropy'] * NUM_ASPECTS + ['binary_crossentropy'] + ['categorical_crossentropy'] * NUM_ASPECTS,
              metrics=[f1_score] * NUM_ASPECTS + [binary_f1_score] + [f1_score] * NUM_ASPECTS)

# Print model summary
model.summary()


def compute_fbeta(y_true, y_pred, beta=0.5):
    # Convert tensors to binary class matrices
    y_true_class = tf.argmax(y_true, axis=-1)
    y_pred_class = tf.argmax(y_pred, axis=-1)

    # Compute TP, FP, FN
    TP = tf.math.count_nonzero(y_pred_class * y_true_class, axis=0)
    FP = tf.math.count_nonzero(y_pred_class * (y_true_class - 1), axis=0)
    FN = tf.math.count_nonzero((y_pred_class - 1) * y_true_class, axis=0)

    # Compute Precision and Recall
    precision = TP / (TP + FP + tf.keras.backend.epsilon())
    recall = TP / (TP + FN + tf.keras.backend.epsilon())

    # Compute F-beta
    fbeta = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall + tf.keras.backend.epsilon())

    # Return the mean F-beta score for the batch
    return tf.reduce_mean(fbeta)


def compute_fbeta(y_true, y_pred, beta=0.5):
    # Cast y_true to the same dtype as y_pred
    y_true = tf.cast(y_true, y_pred.dtype)

    # Convert tensors to binary class matrices
    y_true_class = tf.argmax(y_true, axis=-1)
    y_pred_class = tf.argmax(y_pred, axis=-1)

    # Convert these to float32 for the upcoming computations
    y_true_class = tf.cast(y_true_class, tf.float32)
    y_pred_class = tf.cast(y_pred_class, tf.float32)

    # Compute TP, FP, FN
    TP = tf.math.count_nonzero(y_pred_class * y_true_class, axis=0)
    FP = tf.math.count_nonzero(y_pred_class * (y_true_class - 1), axis=0)
    FN = tf.math.count_nonzero((y_pred_class - 1) * y_true_class, axis=0)

    # Convert TP, FP, FN to float32
    TP = tf.cast(TP, tf.float32)
    FP = tf.cast(FP, tf.float32)
    FN = tf.cast(FN, tf.float32)

    # Compute Precision and Recall
    precision = TP / (TP + FP + tf.keras.backend.epsilon())
    recall = TP / (TP + FN + tf.keras.backend.epsilon())

    # Compute F-beta
    fbeta = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall + tf.keras.backend.epsilon())

    # Return the mean F-beta score for the batch
    return tf.reduce_mean(fbeta)



=--------------------9076875643576243567890-=
zeroshot bart 
import torch
from transformers import pipeline
from multiprocessing import Pool

# Ensure you have a GPU for faster inference (optional but recommended)
device = 0 if torch.cuda.is_available() else -1

# Create the zero-shot classification pipeline
classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli", device=device)

def classify(text):
    # Define the possible labels for classification
    candidate_labels = ["label1", "label2", "label3"]  # Replace with your labels
    result = classifier(text, candidate_labels)
    return result["labels"][0]  # Return the top label

if __name__ == "__main__":
    texts = ["Your list of texts to classify..."]  # Replace with your list of texts

    # Use multiprocessing to classify texts in parallel
    with Pool(processes=4) as pool:  # Adjust the number of processes as needed
        results = pool.map(classify, texts)

    print(results)








=-=-=-=-=------===================----------------------------===========================---------------
# in mortgage industry, we have appraisal to a house or recommendation memos for multi-family.
# sometimes the agents writing these appraisals or rec memos, or lender narratives, or sponsor narratives use some language that are prohibited, 
# for example they should not describe the desirability of neighborhood, or any subjective high-level categories such as race, gender, religion, familial status, age, income, disability, neighborhood description related to applicant, neighborhood,
# like that makes it biased and adds subjectivity on report regarding the above mentioned high-level categories. 
# for removing these prohibited terms related to above high-level categories, 
# we need to parse the appraisal report into sentence and them classified each sentence into one of above multi_categories or none of them if the sentence does not contain none of them. 
# for example following phrases such as good neighborhood, Muslim community , potential district, immigrant community, Chinese neighborhood, black street, good school, kids neighborhood, standards neighborhood, poor subdivision, gentrified neighborhood, fast growing district, gay applicant, young owner , ancient landlord, single loan applicant, 24 years old owner, ... 
# that are implicitly has some meaning related to description of neighborhood, applicant, owner,  ... . 

multi_categories = ['race / ethnicity / national origin / color skin / ... descriptors',
'religion',
'gender identity / sexual orientation / sex / ... descriptors',
'familial status / marital status / familial descriptors',
'neighborhood / community / district / block / subdivision / town / zone / urban / rural / suburban / village / tenants / families / enclave / pockets / ... descriptors',
'age demographics / age descriptors',
'income descriptors and financial status',
'disability descriptors',
'None of above']

binary_category = ['baised', 'not_baised']

# for each sentence in the report, we need to classify it into one of above categories or none of them.
# for example:
# sentence = 'the neighborhood is good'
# category = 'neighborhood / community / district / block / subdivision / town / zone / urban / rural / suburban / village / tenants / families / enclave / pockets / ... descriptors'

# load the a dataframe contains columns such as document name, text, and their categories and binary category from a csv file
import pandas as pd
import torch
import re
import spacy
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from torch.utils.data import DataLoader
import torch
import torch.optim as optim

df = pd.read_csv('MF_prohibited.csv')
df.head()

# we need to clean the text column by removing white space and replace them with space, remove urls, non-ascii characters. 
# we need to convert entities in the text into date_, loc_, org_, person_, time_, money_, percent_, gpe_ , etc. 
# write the function with regex for above rules and apply them on text column.
def text_processing():
    # remove white space
    text = re.sub(r'\s+', ' ', text)
    # remove urls
    text = re.sub(r"http\S+", "", text)
    # remove non-ascii characters
    text = ''.join([i if ord(i) < 128 else ' ' for i in text])
    # convert entities into date_, loc_, org_, person_, time_, money_, percent_, gpe_ , etc.
    doc = nlp(text)
    for ent in doc.ents:
        text = text.replace(ent.text, ent.label_)
    # ...
    return text

df.text.apply(lambda x: text_processing(x))

# load the following huggingface transformers and sentence transformers models for embedding the text into vectors 
# bert-base-uncased, bart-large-mnli, sentence transformers all-MiniLM-L6-V2

model_name1 = 'bert-base-uncased'
model_name2 = 'bart-large-mnli'
model_name3 = 'setence-transformers/all-MiniLM-L6-V2'
tokenizer1 = AutoTokenizer.from_pretrained(model_name1)
tokenizer2 = AutoTokenizer.from_pretrained(model_name2)
tokenizer3 = AutoTokenizer.from_pretrained(model_name3)
model1 = AutoModel.from_pretrained(model_name1)
model2 = AutoModel.from_pretrained(model_name2)
model3 = AutoModel.from_pretrained(model_name3)

# convert the text into sentences, and then use the above models to embed the sentences into vectors
# load the following models for classification
# bert-base-uncased, bart-large-mnli, sentence transformers all-MiniLM-L6-V2
# and then fine-tune them on the above data

def convert_text_into_sentences(text):
    # convert the text into sentences
    # use sentence transformers to parse the text into sentences
    # use spacy en-web-core-trf model to parse the text into sentences
    nlp = spacy.load("en_core_web_trf")
    doc = nlp(text)
    sentences = [sent.text for sent in doc.sents]

    return sentences

# use input_attention_mask and model output to average and max pooling the outputs with tensor size of 128
# write a max_pooling and average_pooling function, the input is model_output and input_attention_mask
def pooling(input_attention_mask, model_output):
    # use input_attention_mask to mask the model_output
    # use max pooling and average pooling to convert the model_output into a single vector representation for the entire sequence (sentence)
    input_attention_mask = input_attention_mask.unsqueeze(-1).expand(model_output.size()).float()
    model_output = model_output * input_attention_mask
    max_pooling = torch.max(model_output, 1)[0]
    average_pooling = torch.mean(model_output, 1)
    # concate both max pooling and average pooling
    pooling = torch.cat((max_pooling, average_pooling), 1)
    return pooling

def pool_embeddings(embeddings, attention_mask):
    # Max pooling
    max_pooled = torch.max(embeddings, dim=1)[0]

    # Average pooling using attention mask
    sum_embeddings = torch.sum(embeddings * attention_mask.unsqueeze(-1), dim=1)
    avg_pooled = sum_embeddings / attention_mask.sum(dim=1, keepdim=True)

    # Concatenate pooled features
    combined = torch.cat([max_pooled, avg_pooled], dim=1)

    return combined

# data object for training the model use def convert_text_into_sentences(text) function
#Pooling strategies are often employed to convert token-level embeddings into a single vector representation for the entire sequence (sentence). Here, we'll use a combination of average and max pooling, taking into account the attention mask to ensure we don't pool over padding tokens.
# use attention mask to ensure we don't pool over padding tokens
class Data(torch.utils.data.Dataset):

    
    def __init__(self, df, tokenizer1, tokenizer2, tokenizer3, model1, model2, model3):
        self.text = df.text.values
        self.sentences = []
        for text in self.text:
            sentences = convert_text_into_sentences(text)
            self.sentences.append(sentences)
        self.categories = df.categories.values
        self.binary_category = df.binary_category.values
        self.tokenizer1 = tokenizer1
        self.tokenizer2 = tokenizer2
        self.tokenizer3 = tokenizer3
        self.model1 = model1
        self.model2 = model2
        self.model3 = model3

    def __len__(self):
        return len(self.text)

    def __getitem__(self, idx):
        text = self.text[idx]
        sentences = self.sentences[idx]
        categories = self.categories[idx]
        binary_category = self.binary_category[idx]

        # embed the sentences into vectors
        embeddings_sentences1 = []
        embeddings_sentences2a = []
        embeddings_sentences2b = []
        embeddings_sentences3 = []
        embeddings_stack = []
        for sentence in sentences:
            # use max-length of sentence to pad the sentence, padding =True, truncation=True, max_length=512            
            input_ids1 = self.tokenizer1(sentence, padding=True, truncation=True, max_length=512, return_tensors="pt")
            input_ids2 = self.tokenizer2(sentence, padding=True, truncation=True, max_length=512, return_tensors="pt")
            input_ids3 = self.tokenizer3(sentence, padding=True, truncation=True, max_length=512, return_tensors="pt")
            with torch.no_grad():
                model_output1 = self.model1(**input_ids1)['last_hidden_state']
                model_output2a = self.model2(**input_ids2)['last_hidden_state']
                model_output2b = self.model2(**input_ids2)['encoder_last_hidden_state']
                model_output3 = self.model3(**input_ids3)['last_hidden_state']
                        
            # use max pooling and average pooling and convert 
            embeddings_sentences1.append(pooling(input_ids1['attention_mask'], model_output1))
            embeddings_sentences2a.append(pooling(input_ids2['attention_mask'], model_output2a))
            embeddings_sentences2b.append(pooling(input_ids2['attention_mask'], model_output2b))
            embeddings_sentences3.append(pooling(input_ids3['attention_mask'], model_output3))
            stack = torch.stack((embeddings_sentences1, embeddings_sentences2a, embeddings_sentences2b, embeddings_sentences3), dim=1)
            embeddings_stack.appedn(torch.mean(stack, dim=1))
                        
        return text, sentences, categories, binary_category, embeddings_stack

# Step 3: Multi-Task Deep Neural Network
# The architecture of the neural network would be:

# An input layer that accepts the concatenated embeddings from the three models.
# A few dense layers for feature extraction.
# Three branches:
# a. Multi-label classification for the high-level categories.
# b. Binary classification for biased/non-biased sentences.
# c. Question-answering task to identify the biased section of the sentence (this is more complex and may require additional considerations).
class MultiTaskModel(nn.Module):
    def __init__(self, embedding_size, num_categories):
        super(MultiTaskModel, self).__init__()

        self.feature_extractor = nn.Sequential(
            nn.Linear(embedding_size, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.5)
        )

        # Multi-label classification branch
        self.multi_label_classifier = nn.Linear(256, num_categories)

        # Binary classification branch
        self.binary_classifier = nn.Linear(256, 1)

        # For QA task, a more sophisticated mechanism is needed. 
        # For the sake of this explanation, I'm using a placeholder.
        self.qa_task = nn.Linear(256, 2)

    def forward(self, x):
        x = self.feature_extractor(x)

        multi_label_output = self.multi_label_classifier(x)
        binary_output = self.binary_classifier(x)
        qa_output = self.qa_task(x)

        return multi_label_output, binary_output, qa_output
    
# Data Preparation
# Assuming you have a PyTorch dataset, you can use the DataLoader for batching
# Instantiate the dataset and dataloader
dataset = AppraisalDataset(your_dataframe, 'text_column_name', 'label_column_name')
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
# Training Loop
# Instantiate the model
model = MultiTaskModel(embedding_size=768*3, num_categories=number_of_high_level_categories)  # Assuming 768 dims for each model's embeddings
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion_multi_label = nn.BCEWithLogitsLoss()  # BCEWithLogitsLoss is typically used for multi-label tasks
criterion_binary = nn.BCEWithLogitsLoss()

num_epochs = 5
for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()

        # Extract embeddings
        embeddings = get_embeddings(batch['text'])
        
        # Forward pass
        multi_label_output, binary_output, _ = model(embeddings)

        # Compute loss
        loss_multi_label = criterion_multi_label(multi_label_output, batch['multi_label_targets'])
        loss_binary = criterion_binary(binary_output, batch['binary_targets'])
        
        # Combine the two losses
        combined_loss = loss_multi_label + loss_binary
        combined_loss.backward()
        
        optimizer.step()
# Note: This is a high-level outline. The actual implementation would require handling various details, such as:
# Moving data and the model to the GPU (if available).
# Tracking and logging metrics.
# Implementing validation and early stopping.
# Adjusting hyperparameters.
# For inference:
def infer(sentence):
    model.eval()
    with torch.no_grad():
        embeddings = get_embeddings(sentence)
        multi_label_pred, binary_pred, qa_pred = model(embeddings)
    
    # Convert predictions to desired format
    # ...

    return multi_label_pred, binary_pred, qa_pred


-----------------------------------------------------------------------------------------------
import torch
from transformers import BartTokenizer, BartForSequenceClassification, BartForQuestionAnswering
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from sklearn.model_selection import train_test_split
import numpy as np

# Define model names and paths
model_name = "facebook/bart-large-mnli"
saved_model_path = "./saved_model"  # Replace with your desired model save path

# Load a pre-trained BART model for sequence classification
tokenizer = BartTokenizer.from_pretrained(model_name)
classification_model = BartForSequenceClassification.from_pretrained(model_name, num_labels=num_categories)

# Sample data
texts = ["Sample text 1", "Sample text 2", "Sample text 3"]  # Replace with your text data
labels = [[1, 0, 1], [0, 1, 0], [1, 1, 0]]  # Replace with your labels, using 1 for positive classes and 0 for others

# Tokenize and format your annotated data for classification
def tokenize_and_format_data(texts, labels):
    input_ids = []
    attention_masks = []
    encoded_labels = []

    for text, label in zip(texts, labels):
        encoding = tokenizer(text, truncation=True, padding=True, return_tensors="pt")
        input_ids.append(encoding.input_ids)
        attention_masks.append(encoding.attention_mask)
        encoded_labels.append(label)

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    encoded_labels = torch.tensor(encoded_labels)

    return input_ids, attention_masks, encoded_labels

input_ids, attention_masks, labels = tokenize_and_format_data(texts, labels)

# Split data into training and validation sets
train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(
    input_ids, attention_masks, labels, test_size=0.2, random_state=42
)

# Create data loaders
batch_size = 32  # Adjust as needed
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)

# Fine-tune the classification model
# You can use the code from previous responses for fine-tuning

# Save the fine-tuned classification model
classification_model.save_pretrained(saved_model_path)
tokenizer.save_pretrained(saved_model_path)

# Step 2: Question Answering

# Load the fine-tuned classification model
qa_model = BartForQuestionAnswering.from_pretrained(saved_model_path)

def find_keywords_and_phrases(texts, class_names):
    keyword_ranges = []

    for text, class_name in zip(texts, class_names):
        # Construct an auxiliary question
        question = f"Which part of the text relates to {class_name}?"

        # Tokenize the text and question
        inputs = tokenizer.encode(question, text, return_tensors="pt", max_length=256, truncation=True)

        # Use the QA model to predict the answer (i.e., keywords or phrases)
        with torch.no_grad():
            outputs = qa_model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask)

        # Extract the predicted answer
        start_index = torch.argmax(outputs.start_logits, dim=1).item()
        end_index = torch.argmax(outputs.end_logits, dim=1).item()

        # Get the corresponding text span
        keyword_span = text[inputs.input_ids[0][start_index:end_index + 1]]

        keyword_ranges.append((class_name, keyword_span))

    return keyword_ranges

# Use the combined model for question answering
classification_model.eval()
keyword_ranges = find_keywords_and_phrases(texts, class_names)

# Print the results
for class_name, keyword_span in keyword_ranges:
    print(f"Class: {class_name}")
    print(f"Keywords/Phrases: {keyword_span}")
    print("\n")

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
import torch
from transformers import BertTokenizer, BertForSequenceClassification, BartTokenizer, BartForQuestionAnswering
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from sklearn.model_selection import train_test_split

# Define model names and paths
classification_model_name = "bert-base-uncased"  # Replace with your classification model name
qa_model_name = "facebook/bart-large-cnn"  # Replace with your QA model name
saved_model_path = "./saved_model"  # Replace with your desired model save path

# Load a pre-trained BERT model for sequence classification
classification_tokenizer = BertTokenizer.from_pretrained(classification_model_name)
classification_model = BertForSequenceClassification.from_pretrained(classification_model_name, num_labels=num_categories)

# Load a pre-trained BART model for question answering
qa_tokenizer = BartTokenizer.from_pretrained(qa_model_name)
qa_model = BartForQuestionAnswering.from_pretrained(qa_model_name)

# Sample data
texts = ["Sample text 1", "Sample text 2", "Sample text 3"]  # Replace with your text data
class_names = ["Class 1", "Class 2", "Class 3"]  # Replace with your class names

# Step 1: Classification

# Tokenize and format your annotated data
def tokenize_and_format_data(texts, labels):
    input_ids = []
    attention_masks = []
    encoded_labels = []

    for text, label in zip(texts, labels):
        encoding = classification_tokenizer(text, truncation=True, padding=True, return_tensors="pt")
        input_ids.append(encoding.input_ids)
        attention_masks.append(encoding.attention_mask)
        encoded_labels.append(label)

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    encoded_labels = torch.tensor(encoded_labels)

    return input_ids, attention_masks, encoded_labels

input_ids, attention_masks, labels = tokenize_and_format_data(texts, class_names)

# Split data into training and validation sets
train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(
    input_ids, attention_masks, labels, test_size=0.2, random_state=42
)

# Create data loaders
batch_size = 32  # Adjust as needed
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)

# Fine-tune the classification model (use the code from previous responses)

# Step 2: Question Answering

def find_keywords_and_phrases(texts, class_names):
    keyword_ranges = []

    for text, class_name in zip(texts, class_names):
        # Tokenize the text and class name
        inputs = qa_tokenizer(class_name, text, return_tensors="pt")

        # Use the QA model to predict the answer (i.e., keywords or phrases)
        with torch.no_grad():
            outputs = qa_model(**inputs)

        # Extract the start and end positions of the answer
        start_index = torch.argmax(outputs.start_logits, dim=1).item()
        end_index = torch.argmax(outputs.end_logits, dim=1).item()

        # Get the corresponding text span
        keyword_span = text[inputs.input_ids[0][start_index:end_index + 1]]

        keyword_ranges.append((class_name, keyword_span))

    return keyword_ranges

# Use the combined model
for epoch in range(num_epochs):  # You can define num_epochs as needed
    classification_model.train()
    total_loss = 0

    for batch in train_dataloader:
        inputs, masks, labels = batch
        outputs = classification_model(inputs, attention_mask=masks, labels=labels)
        loss = outputs.loss

        loss.backward()
        total_loss += loss.item()

        torch.nn.utils.clip_grad_norm_(classification_model.parameters(), max_grad_norm)

        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    # Calculate average training loss for this epoch
    avg_train_loss = total_loss / len(train_dataloader)

    # Validation
    classification_model.eval()
    val_loss = 0

    for batch in val_dataloader:
        inputs, masks, labels = batch
        with torch.no_grad():
            outputs = classification_model(inputs, attention_mask=masks, labels=labels)
            val_loss += outputs.loss.item()

    # Calculate average validation loss for this epoch
    avg_val_loss = val_loss / len(val_dataloader)

    # Print training and validation loss for this epoch
    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')

    # Step 2: Question Answering
    keyword_ranges = find_keywords_and_phrases(texts, class_names)

    # Print the results
    for class_name, keyword_span in keyword_ranges:
        print(f"Class: {class_name}")
        print(f"Keywords/Phrases: {keyword_span}")
        print("\n")

# Save the combined model
classification_model.save_pretrained(saved_model_path)
qa_model.save_pretrained(saved_model_path)
classification_tokenizer.save_pretrained(saved_model_path)
qa_tokenizer.save_pretrained(saved_model_path)

---------------------------------------------------------------------------------------------
from transformers import BartForSequenceClassification, BartTokenizer

model_name = "facebook/bart-large-mnli"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForSequenceClassification.from_pretrained(model_name, num_labels=num_categories)
def tokenize_and_format_data(texts, labels):
    formatted_data = []

    for text, label in zip(texts, labels):
        inputs = tokenizer.encode("classification: " + text, truncation=True, padding=True, return_tensors="pt")
        formatted_data.append((inputs, label))

    return formatted_data

formatted_data = tokenize_and_format_data(texts, labels)
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset

train_data, val_data = train_test_split(formatted_data, test_size=0.2, random_state=42)

train_inputs = torch.stack([item[0] for item in train_data])
train_labels = torch.tensor([item[1] for item in train_data])

val_inputs = torch.stack([item[0] for item in val_data])
val_labels = torch.tensor([item[1] for item in val_data])

train_dataset = TensorDataset(train_inputs, train_labels)
train_sampler = RandomSampler(train_dataset)
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)

val_dataset = TensorDataset(val_inputs, val_labels)
val_sampler = SequentialSampler(val_dataset)
val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)


from transformers import AdamW, get_linear_schedule_with_warmup

# Define optimizer and learning rate scheduler
optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)

# Training loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch in train_dataloader:
        inputs, labels = batch
        outputs = model(inputs, labels=labels)
        loss = outputs.loss

        loss.backward()
        total_loss += loss.item()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)

        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    # Calculate average training loss for this epoch
    avg_train_loss = total_loss / len(train_dataloader)

    # Validation
    model.eval()
    val_loss = 0

    for batch in val_dataloader:
        inputs, labels = batch
        with torch.no_grad():
            outputs = model(inputs, labels=labels)
            val_loss += outputs.loss.item()

    # Calculate average validation loss for this epoch
    avg_val_loss = val_loss / len(val_dataloader)

    # Print training and validation loss for this epoch
    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')


model.save_pretrained(saved_model_path)
tokenizer.save_pretrained(saved_model_path)

=============================================================================================

from transformers import BertTokenizer, BertForSequenceClassification

# Load the pre-trained model and tokenizer
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_categories)
# Tokenize and format your data
def tokenize_and_format_data(texts, labels):
    input_ids = []
    attention_masks = []
    encoded_labels = []

    for text, label in zip(texts, labels):
        encoding = tokenizer(text, truncation=True, padding=True, return_tensors="pt")
        input_ids.append(encoding.input_ids)
        attention_masks.append(encoding.attention_mask)
        encoded_labels.append(label)

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    encoded_labels = torch.tensor(encoded_labels)

    return input_ids, attention_masks, encoded_labels

input_ids, attention_masks, labels = tokenize_and_format_data(texts, labels)
from sklearn.model_selection import train_test_split

train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(
    input_ids, attention_masks, labels, test_size=0.2, random_state=42
)
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)
from transformers import AdamW, get_linear_schedule_with_warmup

# Define optimizer and learning rate scheduler
optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)

# Training loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    
    for batch in train_dataloader:
        # Forward pass
        inputs, masks, labels = batch
        outputs = model(inputs, attention_mask=masks, labels=labels)
        loss = outputs.loss
        
        # Backward pass
        loss.backward()
        total_loss += loss.item()
        
        # Gradient clipping if needed
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        
        # Update parameters
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    # Calculate average training loss for this epoch
    avg_train_loss = total_loss / len(train_dataloader)

    # Validation
    model.eval()
    val_loss = 0

    for batch in val_dataloader:
        inputs, masks, labels = batch
        with torch.no_grad():
            outputs = model(inputs, attention_mask=masks, labels=labels)
            val_loss += outputs.loss.item()

    # Calculate average validation loss for this epoch
    avg_val_loss = val_loss / len(val_dataloader)

    # Print training and validation loss for this epoch
    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')
model.save_pretrained(saved_model_path)
tokenizer.save_pretrained(saved_model_path)




-======================-----------------------------------------------------------------------==================================
import pandas as pd

# Example DataFrame
data = {
    'Column1': ['data1', 'data2'],  # Non-list column
    'Column2': ['info1', 'info2'],  # Non-list column
    'ListColumn1': [[1, 2, 3], [4, 5, 6]],  # List column
    'ListColumn2': [['A', 'B', 'C'], ['D', 'E', 'F']],  # List column
    'ListColumn3': [['X', 'Y', 'Z'], ['U', 'V', 'W']]   # List column
}

df = pd.DataFrame(data)

# Exploding the DataFrame
df_exploded = pd.DataFrame({
    'Column1': df['Column1'].repeat(df['ListColumn1'].str.len()),
    'Column2': df['Column2'].repeat(df['ListColumn1'].str.len()),
    'ListColumn1': [item for sublist in df['ListColumn1'] for item in sublist],
    'ListColumn2': [item for sublist in df['ListColumn2'] for item in sublist],
    'ListColumn3': [item for sublist in df['ListColumn3'] for item in sublist]
}).reset_index(drop=True)

print(df_exploded)

-----------------------------------------------------------------------------------------------------------
data = {
    'Column1': [[1, 2, 3], [4, 5, 6]],
    'Column2': [['a', 'b', 'c'], ['d', 'e', 'f']],
    'Column3': [['x', 'y', 'z'], ['u', 'v', 'w']]
}

df = pd.DataFrame(data)

# Ensure all lists in each row are of the same length
# Explode the DataFrame
df['combined'] = list(zip(df['Column1'], df['Column2'], df['Column3']))
df_exploded = df.explode('combined')

# Create separate columns from the tuples
df_exploded[['Column1', 'Column2', 'Column3']] = pd.DataFrame(df_exploded['combined'].tolist(), index=df_exploded.index)

# Drop the combined column if not needed
df_exploded = df_exploded.drop('combined', axis=1)

print(df_exploded)
