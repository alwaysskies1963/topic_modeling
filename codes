def find_similar_words(input_text, top_n=10):
    # Process the input text with the spaCy transformer model
    doc = trf_model(input_text)
    
    # Initialize a list to store similar words and phrases
    similar_words = []
    
    # Iterate through tokens in the processed text
    for token in doc:
        # Check if the token has vector representation (e.g., excluding stopwords)
        if token.has_vector:
            # Calculate the similarity score between the input token and all tokens in the vocabulary
            similarity_scores = [token.similarity(other_token) for other_token in doc]
            
            # Sort tokens by similarity score in descending order
            sorted_tokens = sorted(enumerate(similarity_scores), key=lambda x: x[1], reverse=True)
            
            # Extract the top N similar tokens (excluding the input token itself)
            similar_tokens = [doc[i[0]].text for i in sorted_tokens if i[0] != token.i][:top_n]
            
            # Add the similar tokens to the list
            similar_words.extend(similar_tokens)
    
    # Remove duplicates from the list
    similar_words = list(set(similar_words))
    
    return similar_words



import spacy

nlp = spacy.load("en_core_web_md")  # Load the medium-sized English model

def find_similar(keyword, threshold=0.89):
    """Find words similar to keyword using spaCy word vectors"""
    
    doc = nlp(keyword)
    query_vector = doc.vector

    similar_words = []
    for word in nlp.vocab:
        if word.has_vector:  # Check if the word has a vector representation
            similarity = query_vector.dot(word.vector)  # Calculate cosine similarity
            if similarity > threshold:
                similar_words.append((word.text, similarity))
    
    similar_words = sorted(similar_words, key=lambda x: x[1], reverse=True)

    print(f"Similar words to {keyword} with similarity > {threshold}:")
    for word, score in similar_words:
        print(f"- {word} ({score:.3f})")

# Example usage:
keyword = "house"
find_similar(keyword)


-------------------------------------------------------------
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel

# Constants
MAX_LEN = 512
EMBEDDING_DIM = 8000
NUM_ASPECTS = 9
DROPOUT_RATE = 0.5

# Custom attention layer
class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1),
                                 initializer="normal")
        self.b = self.add_weight(name="att_bias", shape=(input_shape[1], 1),
                                 initializer="zeros")
        super(AttentionLayer, self).build(input_shape)

    def call(self, x):
        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)
        a = tf.keras.backend.softmax(e, axis=1)
        output = x * a
        return tf.keras.backend.sum(output, axis=1)

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[-1])

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Input Layer
input_text = tf.keras.layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name="input_text")

# BERT for contextual embeddings
bert_model = TFBertModel.from_pretrained('bert-base-uncased')
bert_output = bert_model(input_text)[0]

# Attention layer
attention_output = AttentionLayer()(bert_output)

# Dropout layer after attention
attention_output = tf.keras.layers.Dropout(DROPOUT_RATE)(attention_output)

# Custom layer to concatenate attention outputs to desired dimension
concat_layer = tf.keras.layers.Dense(EMBEDDING_DIM, activation='relu')(attention_output)
concat_layer = tf.keras.layers.Dropout(DROPOUT_RATE)(concat_layer)

# ... [Rest of the model remains the same]

# Print model summary
model.summary()



-=--=========================
# Compile the model with F1-Score as a metric
f1_score = tfa.metrics.F1Score(num_classes=3, average='macro')  # For multi-class tasks
binary_f1_score = tfa.metrics.F1Score(num_classes=1, threshold=0.5)  # For binary tasks

model.compile(optimizer='adam', 
              loss=['categorical_crossentropy'] * NUM_ASPECTS + ['binary_crossentropy'] + ['categorical_crossentropy'] * NUM_ASPECTS,
              metrics=[f1_score] * NUM_ASPECTS + [binary_f1_score] + [f1_score] * NUM_ASPECTS)

# Print model summary
model.summary()












=-=-=-=-=------===================----------------------------===========================---------------
# in mortgage industry, we have appraisal to a house or recommendation memos for multi-family.
# sometimes the agents writing these appraisals or rec memos, or lender narratives, or sponsor narratives use some language that are prohibited, 
# for example they should not describe the desirability of neighborhood, or any subjective high-level categories such as race, gender, religion, familial status, age, income, disability, neighborhood description related to applicant, neighborhood,
# like that makes it biased and adds subjectivity on report regarding the above mentioned high-level categories. 
# for removing these prohibited terms related to above high-level categories, 
# we need to parse the appraisal report into sentence and them classified each sentence into one of above multi_categories or none of them if the sentence does not contain none of them. 
# for example following phrases such as good neighborhood, Muslim community , potential district, immigrant community, Chinese neighborhood, black street, good school, kids neighborhood, standards neighborhood, poor subdivision, gentrified neighborhood, fast growing district, gay applicant, young owner , ancient landlord, single loan applicant, 24 years old owner, ... 
# that are implicitly has some meaning related to description of neighborhood, applicant, owner,  ... . 

multi_categories = ['race / ethnicity / national origin / color skin / ... descriptors',
'religion',
'gender identity / sexual orientation / sex / ... descriptors',
'familial status / marital status / familial descriptors',
'neighborhood / community / district / block / subdivision / town / zone / urban / rural / suburban / village / tenants / families / enclave / pockets / ... descriptors',
'age demographics / age descriptors',
'income descriptors and financial status',
'disability descriptors',
'None of above']

binary_category = ['baised', 'not_baised']

# for each sentence in the report, we need to classify it into one of above categories or none of them.
# for example:
# sentence = 'the neighborhood is good'
# category = 'neighborhood / community / district / block / subdivision / town / zone / urban / rural / suburban / village / tenants / families / enclave / pockets / ... descriptors'

# load the a dataframe contains columns such as document name, text, and their categories and binary category from a csv file
import pandas as pd
import torch
import re
import spacy
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from torch.utils.data import DataLoader
import torch
import torch.optim as optim

df = pd.read_csv('MF_prohibited.csv')
df.head()

# we need to clean the text column by removing white space and replace them with space, remove urls, non-ascii characters. 
# we need to convert entities in the text into date_, loc_, org_, person_, time_, money_, percent_, gpe_ , etc. 
# write the function with regex for above rules and apply them on text column.
def text_processing():
    # remove white space
    text = re.sub(r'\s+', ' ', text)
    # remove urls
    text = re.sub(r"http\S+", "", text)
    # remove non-ascii characters
    text = ''.join([i if ord(i) < 128 else ' ' for i in text])
    # convert entities into date_, loc_, org_, person_, time_, money_, percent_, gpe_ , etc.
    doc = nlp(text)
    for ent in doc.ents:
        text = text.replace(ent.text, ent.label_)
    # ...
    return text

df.text.apply(lambda x: text_processing(x))

# load the following huggingface transformers and sentence transformers models for embedding the text into vectors 
# bert-base-uncased, bart-large-mnli, sentence transformers all-MiniLM-L6-V2

model_name1 = 'bert-base-uncased'
model_name2 = 'bart-large-mnli'
model_name3 = 'setence-transformers/all-MiniLM-L6-V2'
tokenizer1 = AutoTokenizer.from_pretrained(model_name1)
tokenizer2 = AutoTokenizer.from_pretrained(model_name2)
tokenizer3 = AutoTokenizer.from_pretrained(model_name3)
model1 = AutoModel.from_pretrained(model_name1)
model2 = AutoModel.from_pretrained(model_name2)
model3 = AutoModel.from_pretrained(model_name3)

# convert the text into sentences, and then use the above models to embed the sentences into vectors
# load the following models for classification
# bert-base-uncased, bart-large-mnli, sentence transformers all-MiniLM-L6-V2
# and then fine-tune them on the above data

def convert_text_into_sentences(text):
    # convert the text into sentences
    # use sentence transformers to parse the text into sentences
    # use spacy en-web-core-trf model to parse the text into sentences
    nlp = spacy.load("en_core_web_trf")
    doc = nlp(text)
    sentences = [sent.text for sent in doc.sents]

    return sentences

# use input_attention_mask and model output to average and max pooling the outputs with tensor size of 128
# write a max_pooling and average_pooling function, the input is model_output and input_attention_mask
def pooling(input_attention_mask, model_output):
    # use input_attention_mask to mask the model_output
    # use max pooling and average pooling to convert the model_output into a single vector representation for the entire sequence (sentence)
    input_attention_mask = input_attention_mask.unsqueeze(-1).expand(model_output.size()).float()
    model_output = model_output * input_attention_mask
    max_pooling = torch.max(model_output, 1)[0]
    average_pooling = torch.mean(model_output, 1)
    # concate both max pooling and average pooling
    pooling = torch.cat((max_pooling, average_pooling), 1)
    return pooling

def pool_embeddings(embeddings, attention_mask):
    # Max pooling
    max_pooled = torch.max(embeddings, dim=1)[0]

    # Average pooling using attention mask
    sum_embeddings = torch.sum(embeddings * attention_mask.unsqueeze(-1), dim=1)
    avg_pooled = sum_embeddings / attention_mask.sum(dim=1, keepdim=True)

    # Concatenate pooled features
    combined = torch.cat([max_pooled, avg_pooled], dim=1)

    return combined

# data object for training the model use def convert_text_into_sentences(text) function
#Pooling strategies are often employed to convert token-level embeddings into a single vector representation for the entire sequence (sentence). Here, we'll use a combination of average and max pooling, taking into account the attention mask to ensure we don't pool over padding tokens.
# use attention mask to ensure we don't pool over padding tokens
class Data(torch.utils.data.Dataset):

    
    def __init__(self, df, tokenizer1, tokenizer2, tokenizer3, model1, model2, model3):
        self.text = df.text.values
        self.sentences = []
        for text in self.text:
            sentences = convert_text_into_sentences(text)
            self.sentences.append(sentences)
        self.categories = df.categories.values
        self.binary_category = df.binary_category.values
        self.tokenizer1 = tokenizer1
        self.tokenizer2 = tokenizer2
        self.tokenizer3 = tokenizer3
        self.model1 = model1
        self.model2 = model2
        self.model3 = model3

    def __len__(self):
        return len(self.text)

    def __getitem__(self, idx):
        text = self.text[idx]
        sentences = self.sentences[idx]
        categories = self.categories[idx]
        binary_category = self.binary_category[idx]

        # embed the sentences into vectors
        embeddings_sentences1 = []
        embeddings_sentences2a = []
        embeddings_sentences2b = []
        embeddings_sentences3 = []
        embeddings_stack = []
        for sentence in sentences:
            # use max-length of sentence to pad the sentence, padding =True, truncation=True, max_length=512            
            input_ids1 = self.tokenizer1(sentence, padding=True, truncation=True, max_length=512, return_tensors="pt")
            input_ids2 = self.tokenizer2(sentence, padding=True, truncation=True, max_length=512, return_tensors="pt")
            input_ids3 = self.tokenizer3(sentence, padding=True, truncation=True, max_length=512, return_tensors="pt")
            with torch.no_grad():
                model_output1 = self.model1(**input_ids1)['last_hidden_state']
                model_output2a = self.model2(**input_ids2)['last_hidden_state']
                model_output2b = self.model2(**input_ids2)['encoder_last_hidden_state']
                model_output3 = self.model3(**input_ids3)['last_hidden_state']
                        
            # use max pooling and average pooling and convert 
            embeddings_sentences1.append(pooling(input_ids1['attention_mask'], model_output1))
            embeddings_sentences2a.append(pooling(input_ids2['attention_mask'], model_output2a))
            embeddings_sentences2b.append(pooling(input_ids2['attention_mask'], model_output2b))
            embeddings_sentences3.append(pooling(input_ids3['attention_mask'], model_output3))
            stack = torch.stack((embeddings_sentences1, embeddings_sentences2a, embeddings_sentences2b, embeddings_sentences3), dim=1)
            embeddings_stack.appedn(torch.mean(stack, dim=1))
                        
        return text, sentences, categories, binary_category, embeddings_stack

# Step 3: Multi-Task Deep Neural Network
# The architecture of the neural network would be:

# An input layer that accepts the concatenated embeddings from the three models.
# A few dense layers for feature extraction.
# Three branches:
# a. Multi-label classification for the high-level categories.
# b. Binary classification for biased/non-biased sentences.
# c. Question-answering task to identify the biased section of the sentence (this is more complex and may require additional considerations).
class MultiTaskModel(nn.Module):
    def __init__(self, embedding_size, num_categories):
        super(MultiTaskModel, self).__init__()

        self.feature_extractor = nn.Sequential(
            nn.Linear(embedding_size, 512),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.5)
        )

        # Multi-label classification branch
        self.multi_label_classifier = nn.Linear(256, num_categories)

        # Binary classification branch
        self.binary_classifier = nn.Linear(256, 1)

        # For QA task, a more sophisticated mechanism is needed. 
        # For the sake of this explanation, I'm using a placeholder.
        self.qa_task = nn.Linear(256, 2)

    def forward(self, x):
        x = self.feature_extractor(x)

        multi_label_output = self.multi_label_classifier(x)
        binary_output = self.binary_classifier(x)
        qa_output = self.qa_task(x)

        return multi_label_output, binary_output, qa_output
    
# Data Preparation
# Assuming you have a PyTorch dataset, you can use the DataLoader for batching
# Instantiate the dataset and dataloader
dataset = AppraisalDataset(your_dataframe, 'text_column_name', 'label_column_name')
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
# Training Loop
# Instantiate the model
model = MultiTaskModel(embedding_size=768*3, num_categories=number_of_high_level_categories)  # Assuming 768 dims for each model's embeddings
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion_multi_label = nn.BCEWithLogitsLoss()  # BCEWithLogitsLoss is typically used for multi-label tasks
criterion_binary = nn.BCEWithLogitsLoss()

num_epochs = 5
for epoch in range(num_epochs):
    for batch in dataloader:
        optimizer.zero_grad()

        # Extract embeddings
        embeddings = get_embeddings(batch['text'])
        
        # Forward pass
        multi_label_output, binary_output, _ = model(embeddings)

        # Compute loss
        loss_multi_label = criterion_multi_label(multi_label_output, batch['multi_label_targets'])
        loss_binary = criterion_binary(binary_output, batch['binary_targets'])
        
        # Combine the two losses
        combined_loss = loss_multi_label + loss_binary
        combined_loss.backward()
        
        optimizer.step()
# Note: This is a high-level outline. The actual implementation would require handling various details, such as:
# Moving data and the model to the GPU (if available).
# Tracking and logging metrics.
# Implementing validation and early stopping.
# Adjusting hyperparameters.
# For inference:
def infer(sentence):
    model.eval()
    with torch.no_grad():
        embeddings = get_embeddings(sentence)
        multi_label_pred, binary_pred, qa_pred = model(embeddings)
    
    # Convert predictions to desired format
    # ...

    return multi_label_pred, binary_pred, qa_pred


-----------------------------------------------------------------------------------------------
import torch
from transformers import BartTokenizer, BartForSequenceClassification, BartForQuestionAnswering
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from sklearn.model_selection import train_test_split
import numpy as np

# Define model names and paths
model_name = "facebook/bart-large-mnli"
saved_model_path = "./saved_model"  # Replace with your desired model save path

# Load a pre-trained BART model for sequence classification
tokenizer = BartTokenizer.from_pretrained(model_name)
classification_model = BartForSequenceClassification.from_pretrained(model_name, num_labels=num_categories)

# Sample data
texts = ["Sample text 1", "Sample text 2", "Sample text 3"]  # Replace with your text data
labels = [[1, 0, 1], [0, 1, 0], [1, 1, 0]]  # Replace with your labels, using 1 for positive classes and 0 for others

# Tokenize and format your annotated data for classification
def tokenize_and_format_data(texts, labels):
    input_ids = []
    attention_masks = []
    encoded_labels = []

    for text, label in zip(texts, labels):
        encoding = tokenizer(text, truncation=True, padding=True, return_tensors="pt")
        input_ids.append(encoding.input_ids)
        attention_masks.append(encoding.attention_mask)
        encoded_labels.append(label)

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    encoded_labels = torch.tensor(encoded_labels)

    return input_ids, attention_masks, encoded_labels

input_ids, attention_masks, labels = tokenize_and_format_data(texts, labels)

# Split data into training and validation sets
train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(
    input_ids, attention_masks, labels, test_size=0.2, random_state=42
)

# Create data loaders
batch_size = 32  # Adjust as needed
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)

# Fine-tune the classification model
# You can use the code from previous responses for fine-tuning

# Save the fine-tuned classification model
classification_model.save_pretrained(saved_model_path)
tokenizer.save_pretrained(saved_model_path)

# Step 2: Question Answering

# Load the fine-tuned classification model
qa_model = BartForQuestionAnswering.from_pretrained(saved_model_path)

def find_keywords_and_phrases(texts, class_names):
    keyword_ranges = []

    for text, class_name in zip(texts, class_names):
        # Construct an auxiliary question
        question = f"Which part of the text relates to {class_name}?"

        # Tokenize the text and question
        inputs = tokenizer.encode(question, text, return_tensors="pt", max_length=256, truncation=True)

        # Use the QA model to predict the answer (i.e., keywords or phrases)
        with torch.no_grad():
            outputs = qa_model(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask)

        # Extract the predicted answer
        start_index = torch.argmax(outputs.start_logits, dim=1).item()
        end_index = torch.argmax(outputs.end_logits, dim=1).item()

        # Get the corresponding text span
        keyword_span = text[inputs.input_ids[0][start_index:end_index + 1]]

        keyword_ranges.append((class_name, keyword_span))

    return keyword_ranges

# Use the combined model for question answering
classification_model.eval()
keyword_ranges = find_keywords_and_phrases(texts, class_names)

# Print the results
for class_name, keyword_span in keyword_ranges:
    print(f"Class: {class_name}")
    print(f"Keywords/Phrases: {keyword_span}")
    print("\n")

-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
import torch
from transformers import BertTokenizer, BertForSequenceClassification, BartTokenizer, BartForQuestionAnswering
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from sklearn.model_selection import train_test_split

# Define model names and paths
classification_model_name = "bert-base-uncased"  # Replace with your classification model name
qa_model_name = "facebook/bart-large-cnn"  # Replace with your QA model name
saved_model_path = "./saved_model"  # Replace with your desired model save path

# Load a pre-trained BERT model for sequence classification
classification_tokenizer = BertTokenizer.from_pretrained(classification_model_name)
classification_model = BertForSequenceClassification.from_pretrained(classification_model_name, num_labels=num_categories)

# Load a pre-trained BART model for question answering
qa_tokenizer = BartTokenizer.from_pretrained(qa_model_name)
qa_model = BartForQuestionAnswering.from_pretrained(qa_model_name)

# Sample data
texts = ["Sample text 1", "Sample text 2", "Sample text 3"]  # Replace with your text data
class_names = ["Class 1", "Class 2", "Class 3"]  # Replace with your class names

# Step 1: Classification

# Tokenize and format your annotated data
def tokenize_and_format_data(texts, labels):
    input_ids = []
    attention_masks = []
    encoded_labels = []

    for text, label in zip(texts, labels):
        encoding = classification_tokenizer(text, truncation=True, padding=True, return_tensors="pt")
        input_ids.append(encoding.input_ids)
        attention_masks.append(encoding.attention_mask)
        encoded_labels.append(label)

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    encoded_labels = torch.tensor(encoded_labels)

    return input_ids, attention_masks, encoded_labels

input_ids, attention_masks, labels = tokenize_and_format_data(texts, class_names)

# Split data into training and validation sets
train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(
    input_ids, attention_masks, labels, test_size=0.2, random_state=42
)

# Create data loaders
batch_size = 32  # Adjust as needed
train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)

# Fine-tune the classification model (use the code from previous responses)

# Step 2: Question Answering

def find_keywords_and_phrases(texts, class_names):
    keyword_ranges = []

    for text, class_name in zip(texts, class_names):
        # Tokenize the text and class name
        inputs = qa_tokenizer(class_name, text, return_tensors="pt")

        # Use the QA model to predict the answer (i.e., keywords or phrases)
        with torch.no_grad():
            outputs = qa_model(**inputs)

        # Extract the start and end positions of the answer
        start_index = torch.argmax(outputs.start_logits, dim=1).item()
        end_index = torch.argmax(outputs.end_logits, dim=1).item()

        # Get the corresponding text span
        keyword_span = text[inputs.input_ids[0][start_index:end_index + 1]]

        keyword_ranges.append((class_name, keyword_span))

    return keyword_ranges

# Use the combined model
for epoch in range(num_epochs):  # You can define num_epochs as needed
    classification_model.train()
    total_loss = 0

    for batch in train_dataloader:
        inputs, masks, labels = batch
        outputs = classification_model(inputs, attention_mask=masks, labels=labels)
        loss = outputs.loss

        loss.backward()
        total_loss += loss.item()

        torch.nn.utils.clip_grad_norm_(classification_model.parameters(), max_grad_norm)

        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    # Calculate average training loss for this epoch
    avg_train_loss = total_loss / len(train_dataloader)

    # Validation
    classification_model.eval()
    val_loss = 0

    for batch in val_dataloader:
        inputs, masks, labels = batch
        with torch.no_grad():
            outputs = classification_model(inputs, attention_mask=masks, labels=labels)
            val_loss += outputs.loss.item()

    # Calculate average validation loss for this epoch
    avg_val_loss = val_loss / len(val_dataloader)

    # Print training and validation loss for this epoch
    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')

    # Step 2: Question Answering
    keyword_ranges = find_keywords_and_phrases(texts, class_names)

    # Print the results
    for class_name, keyword_span in keyword_ranges:
        print(f"Class: {class_name}")
        print(f"Keywords/Phrases: {keyword_span}")
        print("\n")

# Save the combined model
classification_model.save_pretrained(saved_model_path)
qa_model.save_pretrained(saved_model_path)
classification_tokenizer.save_pretrained(saved_model_path)
qa_tokenizer.save_pretrained(saved_model_path)

---------------------------------------------------------------------------------------------
from transformers import BartForSequenceClassification, BartTokenizer

model_name = "facebook/bart-large-mnli"
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForSequenceClassification.from_pretrained(model_name, num_labels=num_categories)
def tokenize_and_format_data(texts, labels):
    formatted_data = []

    for text, label in zip(texts, labels):
        inputs = tokenizer.encode("classification: " + text, truncation=True, padding=True, return_tensors="pt")
        formatted_data.append((inputs, label))

    return formatted_data

formatted_data = tokenize_and_format_data(texts, labels)
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset

train_data, val_data = train_test_split(formatted_data, test_size=0.2, random_state=42)

train_inputs = torch.stack([item[0] for item in train_data])
train_labels = torch.tensor([item[1] for item in train_data])

val_inputs = torch.stack([item[0] for item in val_data])
val_labels = torch.tensor([item[1] for item in val_data])

train_dataset = TensorDataset(train_inputs, train_labels)
train_sampler = RandomSampler(train_dataset)
train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=batch_size)

val_dataset = TensorDataset(val_inputs, val_labels)
val_sampler = SequentialSampler(val_dataset)
val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=batch_size)


from transformers import AdamW, get_linear_schedule_with_warmup

# Define optimizer and learning rate scheduler
optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)

# Training loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch in train_dataloader:
        inputs, labels = batch
        outputs = model(inputs, labels=labels)
        loss = outputs.loss

        loss.backward()
        total_loss += loss.item()

        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)

        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    # Calculate average training loss for this epoch
    avg_train_loss = total_loss / len(train_dataloader)

    # Validation
    model.eval()
    val_loss = 0

    for batch in val_dataloader:
        inputs, labels = batch
        with torch.no_grad():
            outputs = model(inputs, labels=labels)
            val_loss += outputs.loss.item()

    # Calculate average validation loss for this epoch
    avg_val_loss = val_loss / len(val_dataloader)

    # Print training and validation loss for this epoch
    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')


model.save_pretrained(saved_model_path)
tokenizer.save_pretrained(saved_model_path)

=============================================================================================

from transformers import BertTokenizer, BertForSequenceClassification

# Load the pre-trained model and tokenizer
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_categories)
# Tokenize and format your data
def tokenize_and_format_data(texts, labels):
    input_ids = []
    attention_masks = []
    encoded_labels = []

    for text, label in zip(texts, labels):
        encoding = tokenizer(text, truncation=True, padding=True, return_tensors="pt")
        input_ids.append(encoding.input_ids)
        attention_masks.append(encoding.attention_mask)
        encoded_labels.append(label)

    input_ids = torch.cat(input_ids, dim=0)
    attention_masks = torch.cat(attention_masks, dim=0)
    encoded_labels = torch.tensor(encoded_labels)

    return input_ids, attention_masks, encoded_labels

input_ids, attention_masks, labels = tokenize_and_format_data(texts, labels)
from sklearn.model_selection import train_test_split

train_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(
    input_ids, attention_masks, labels, test_size=0.2, random_state=42
)
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

train_data = TensorDataset(train_inputs, train_masks, train_labels)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

val_data = TensorDataset(val_inputs, val_masks, val_labels)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)
from transformers import AdamW, get_linear_schedule_with_warmup

# Define optimizer and learning rate scheduler
optimizer = AdamW(model.parameters(), lr=learning_rate, eps=epsilon)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)

# Training loop
for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    
    for batch in train_dataloader:
        # Forward pass
        inputs, masks, labels = batch
        outputs = model(inputs, attention_mask=masks, labels=labels)
        loss = outputs.loss
        
        # Backward pass
        loss.backward()
        total_loss += loss.item()
        
        # Gradient clipping if needed
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        
        # Update parameters
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

    # Calculate average training loss for this epoch
    avg_train_loss = total_loss / len(train_dataloader)

    # Validation
    model.eval()
    val_loss = 0

    for batch in val_dataloader:
        inputs, masks, labels = batch
        with torch.no_grad():
            outputs = model(inputs, attention_mask=masks, labels=labels)
            val_loss += outputs.loss.item()

    # Calculate average validation loss for this epoch
    avg_val_loss = val_loss / len(val_dataloader)

    # Print training and validation loss for this epoch
    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')
model.save_pretrained(saved_model_path)
tokenizer.save_pretrained(saved_model_path)

